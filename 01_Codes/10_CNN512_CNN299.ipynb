{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the paper, CNN512 with dropout achieves the highest accuracy 0.841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *The hyperparameter configuration of CNNs.*\n",
    "\n",
    "Configuration\tValues\n",
    "\n",
    "Optimizer\tSGD\n",
    "\n",
    "Momentum\t0.9\n",
    "\n",
    "Max Learning rate\t1×10−1 in custom CNNs\n",
    "\n",
    "1×10−2 in EfficientNetB0\n",
    "\n",
    "Base Learning rate\t1×10−4\n",
    "\n",
    "Mode\ttriangular\n",
    "\n",
    "Class weight\tauto\n",
    "\n",
    "Dropout\t0.5\n",
    "\n",
    "Augmentation\t20 times (I didn't include augmentation in the following code since we decide not to do it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *CNN512* \n",
    "\n",
    "Layer\tOperator\tLayer Details\n",
    "\n",
    "Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "\n",
    "Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "\n",
    "Layer 2\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 3\tRelu layer\t-\n",
    "\n",
    "Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "\n",
    "Layer 6\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 7\tRelu layer\t-\n",
    "\n",
    "Layer 8\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 10\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 11\tRelu layer\t-\n",
    "\n",
    "Layer 12\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 14\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 15\tRelu layer\t-\n",
    "\n",
    "Layer 16\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "\n",
    "Layer 18\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 19\tRelu layer\t-\n",
    "\n",
    "Layer 20\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "\n",
    "Layer 22\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 23\tRelu layer\t-\n",
    "\n",
    "Layer 24\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 25\tFlatten layer\t-\n",
    "\n",
    "Layer 26\tFC layer\tNeurons number = 1000\n",
    "\n",
    "Layer 27\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 28\tRelu layer\t-\n",
    "\n",
    "Layer 29\tFC layer\tNeurons number = 500\n",
    "\n",
    "Layer 30\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 31\tRelu layer\t-\n",
    "\n",
    "Layer 32\tFC layer\tWith SoftMax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 512, 3)  # placeholder\n",
    "num_classes = 5\n",
    "epochs = 0  # placeholder\n",
    "x_train = 0  # placeholder\n",
    "y_train = 0  # placeholder\n",
    "x_test = 0  # placeholder\n",
    "y_test = 0  # placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN512 Without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn512_nodropout = models.Sequential(\n",
    "    [\n",
    "        # Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2), input_shape=input_shape),\n",
    "        # Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 2\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 6\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 10\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 14\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 18\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 19\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 20\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "        layers.Conv2D(200, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 22\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 24\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 25\tFlatten layer\t-\n",
    "        layers.Flatten(),\n",
    "        # Layer 26\tFC layer\tNeurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 27\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 28\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # layers.Dropout(0.5),\n",
    "        # Layer 29\tFC layer\tNeurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 30\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 31\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # layers.Dropout(0.5),\n",
    "        # Layer 32\tFC layer with Softmax activatio\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn512_nodropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# fitting the model\n",
    "cnn512_nodropout.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# evaluating the model\n",
    "cnn512_nodropout.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN512 With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn512_dropout = models.Sequential(\n",
    "    [\n",
    "        # Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2), input_shape=input_shape),\n",
    "        # Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 2\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 6\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 10\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 14\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 18\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 19\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 20\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "        layers.Conv2D(200, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 22\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 24\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 25\tFlatten layer\t-\n",
    "        layers.Flatten(),\n",
    "        # Layer 26\tFC layer\tNeurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 27\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 28\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 29\tFC layer\tNeurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 30\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 31\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 32\tFC layer with Softmax activatio\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn512_dropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# fitting the model\n",
    "cnn512_dropout.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# evaluating the model\n",
    "cnn512_dropout.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *CNN299* \n",
    "\n",
    "Layer\tOperator\tLayer Details\n",
    "\n",
    "Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "\n",
    "Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "\n",
    "Layer 2\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 3\tRelu layer\t-\n",
    "\n",
    "Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "\n",
    "Layer 6\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 7\tRelu layer\t-\n",
    "\n",
    "Layer 8\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 10\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 11\tRelu layer\t-\n",
    "\n",
    "Layer 12\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 14\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 15\tRelu layer\t-\n",
    "\n",
    "Layer 16\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 17\tFlatten layer\t-\n",
    "\n",
    "Layer 18\tFC layer\tNeurons number = 1000\n",
    "\n",
    "Layer 19\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 20\tRelu layer\t-\n",
    "\n",
    "Layer 21\tFC layer\tNeurons number = 500\n",
    "\n",
    "Layer 22\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 23\tRelu layer\t-\n",
    "\n",
    "Layer 24\tFC layer\tWith SoftMax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (299, 299, 3)  # placeholder\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN299 Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn299_nodropout = models.Sequential(\n",
    "    [\n",
    "        # Input Layer Zero Padding layer Padding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2), input_shape=input_shape),\n",
    "        # Layer 1 2D CONV layer Kernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 2 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4 Max Pooling layer Pooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5 2D CONV layer Kernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 6 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9 2D CONV layer Kernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 10 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13 2D CONV layer Kernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 14 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17 Flatten Layer -\n",
    "        layers.Flatten(),\n",
    "        # Layer 18 FC layer Neurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 19 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 20 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 21 FC layer Neurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 22 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 24 FC layer with Softmax activation for class predictions\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn299_nodropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# fitting the model\n",
    "cnn299_nodropout.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# evaluating the model\n",
    "cnn299_nodropout.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CNN299 With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn299_dropout = models.Sequential(\n",
    "    [\n",
    "        # Input Layer Zero Padding layer Padding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2), input_shape=input_shape),\n",
    "        # Layer 1 2D CONV layer Kernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 2 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4 Max Pooling layer Pooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5 2D CONV layer Kernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 6 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9 2D CONV layer Kernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 10 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13 2D CONV layer Kernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3), padding=\"same\"),\n",
    "        # Layer 14 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16 Max Pooling layer Pooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17 Flatten Layer -\n",
    "        layers.Flatten(),\n",
    "        # Layer 18 FC layer Neurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 19 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 20 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 21 FC layer Neurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 22 Batch Normalization layer\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23 Relu layer\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 24 FC layer with Softmax activation for class predictions\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn299_dropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# fitting the model\n",
    "cnn299_dropout.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# evaluating the model\n",
    "cnn299_dropout.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
