{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def restart_kernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)\n",
    "\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 04:54:04.336997: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 04:54:04.503101: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-14 04:54:05.342017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 04:54:05.342180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 04:54:05.342202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 04:54:06.705601: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 04:54:06.982749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14791 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable for TensorFlow\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth before initializing GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Handle potential errors here\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Set seeds to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize base model with pre-trained weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model RAW IDRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load the data\n",
    "\n",
    "X_train = np.load(\"../../aws_s3/Idrid_224_Raw/X_train.npy\")\n",
    "y_train = np.load(\"../../aws_s3/Idrid_224_Raw/y_train.npy\")\n",
    "X_test = np.load(\"../../aws_s3/Idrid_224_Raw/X_test.npy\")\n",
    "y_test = np.load(\"../../aws_s3/Idrid_224_Raw/y_test.npy\")\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float32\")\n",
    "# y_train = tf.convert_to_tensor(y_train, dtype=\"float32\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float32\")\n",
    "# y_test = tf.convert_to_tensor(y_test, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a new model on top with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freesze the base model\n",
    "base_model.trainable = False\n",
    "num_classes = 4\n",
    "epochs = 50\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 04:54:18.998880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2024-04-14 04:54:19.103308: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 7s 39ms/step - loss: 1.8416 - accuracy: 0.2114 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.6140 - accuracy: 0.2602 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.6353 - accuracy: 0.3516 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.4381 - accuracy: 0.3516 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.3216 - accuracy: 0.4492 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.4094 - accuracy: 0.4268 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.2302 - accuracy: 0.4268 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.2929 - accuracy: 0.4614 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1215 - accuracy: 0.5163 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.0529 - accuracy: 0.5244 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1358 - accuracy: 0.4675 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0627 - accuracy: 0.4939 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1428 - accuracy: 0.6077 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9883 - accuracy: 0.5041 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0581 - accuracy: 0.6159 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0196 - accuracy: 0.4959 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1491 - accuracy: 0.6362 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9652 - accuracy: 0.5305 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8996 - accuracy: 0.6057 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8345 - accuracy: 0.5996 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9335 - accuracy: 0.6260 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8613 - accuracy: 0.5915 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9583 - accuracy: 0.5610 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.8435 - accuracy: 0.5711 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8518 - accuracy: 0.5671 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.0546 - accuracy: 0.6138 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8856 - accuracy: 0.5346 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9810 - accuracy: 0.6240 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8344 - accuracy: 0.6463 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9001 - accuracy: 0.5833 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0557 - accuracy: 0.5508 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9340 - accuracy: 0.6118 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9237 - accuracy: 0.5467 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1320 - accuracy: 0.5732 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8742 - accuracy: 0.6382 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8692 - accuracy: 0.5976 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.0350 - accuracy: 0.5976 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8290 - accuracy: 0.6016 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7940 - accuracy: 0.6240 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8341 - accuracy: 0.6321 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9948 - accuracy: 0.6118 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9506 - accuracy: 0.6504 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9219 - accuracy: 0.5813 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8482 - accuracy: 0.6463 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8870 - accuracy: 0.5915 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8838 - accuracy: 0.6280 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8858 - accuracy: 0.6159 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9070 - accuracy: 0.5711 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.8477 - accuracy: 0.6667 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8971 - accuracy: 0.5915 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2da11254e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fine tuning: unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   Y          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 5,791,052\n",
      "Non-trainable params: 42,023\n",
      "____________________________________________________________________________\n",
      "Fitting the end-to-end model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 17s 106ms/step - loss: 0.9230 - accuracy: 0.5955 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.9385 - accuracy: 0.5976 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 1.1054 - accuracy: 0.4898 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 1.1402 - accuracy: 0.5122 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 1.0170 - accuracy: 0.5142 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.9253 - accuracy: 0.5874 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.0942 - accuracy: 0.4776 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.2094 - accuracy: 0.5122 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 1.1379 - accuracy: 0.5122 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.0791 - accuracy: 0.4736 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 1.3398 - accuracy: 0.4126 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.9939 - accuracy: 0.5610 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.1260 - accuracy: 0.5061 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 1.1343 - accuracy: 0.4553 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.2398 - accuracy: 0.5285 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.7736 - accuracy: 0.5488 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.7236 - accuracy: 0.6646 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.6646 - accuracy: 0.6911 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.5025 - accuracy: 0.7154 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.4189 - accuracy: 0.7764 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.3521 - accuracy: 0.7825 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.3419 - accuracy: 0.8354 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2995 - accuracy: 0.8150 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.2992 - accuracy: 0.8496 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.3999 - accuracy: 0.7988 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.3063 - accuracy: 0.8394 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1757 - accuracy: 0.9004 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1840 - accuracy: 0.9045 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.6394 - accuracy: 0.7988 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.4545 - accuracy: 0.8252 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.4053 - accuracy: 0.7927 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.6483 - accuracy: 0.7622 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.4896 - accuracy: 0.8089 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.4452 - accuracy: 0.7988 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2611 - accuracy: 0.8699 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.2065 - accuracy: 0.8801 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0806 - accuracy: 0.9553 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0452 - accuracy: 0.9776 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0470 - accuracy: 0.9776 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0754 - accuracy: 0.9675 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0232 - accuracy: 0.9919 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0231 - accuracy: 0.9919 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0162 - accuracy: 0.9919 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0142 - accuracy: 0.9939 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0173 - accuracy: 0.9939 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0156 - accuracy: 0.9939 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0428 - accuracy: 0.9837 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0907 - accuracy: 0.9593 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0321 - accuracy: 0.9858 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0489 - accuracy: 0.9919 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d8ed1e170>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.keras\")\n",
    "# 50 epochs == 41% accuracy\n",
    "# 25 epochs == 45% accuracy\n",
    "# 20 epochs == 45.83% accuracy\n",
    "# 25 epochs == 62.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.4580 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.45804500579834, 0.5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test dataset evaluation\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epcohs, tensor X, 0.4583\n",
    "# 20 epcohs, tensor X, 0.5000\n",
    "# 20 epcohs, tensor X, 0.5000\n",
    "# 20 epcohs, tensor X, 0.5417\n",
    "# 20 epcohs, tensor X, 0.4583\n",
    "# 20 epcohs, tensor X, 0.5833\n",
    "# 20 epcohs, tensor X  0.5417\n",
    "# 20 epcohs, tensor X  0.5417\n",
    "# 20 epcohs, tensor X  0.6667\n",
    "# 20 epcohs, tensor X. 0.5000\n",
    "# 20 epcohs, tensor X. 0.4583\n",
    "# 20 epcohs, tensor X.  0.5000\n",
    "# 20 epcohs, tensor X.  0.5417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 epochs, tensor X, 0.5833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = np.load(\"../../aws_s3/Idrid_224_CN/X_train.npy\")\n",
    "y_train = np.load(\"../../aws_s3/Idrid_224_CN/y_train.npy\")\n",
    "X_test = np.load(\"../../aws_s3/Idrid_224_CN/X_test.npy\")\n",
    "y_test = np.load(\"../../aws_s3/Idrid_224_CN/y_test.npy\")\n",
    "\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float32\")\n",
    "# y_train = tf.convert_to_tensor(y_train, dtype=\"float32\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float32\")\n",
    "# y_test = tf.convert_to_tensor(y_test, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout_2 (Dropout)         (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense_3 (Dense)             (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_3 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_4 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_5 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freesze the base model\n",
    "base_model.trainable = False\n",
    "num_classes = 4\n",
    "epochs = 50\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model_cn = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_cn.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 7s 26ms/step - loss: 1.8780 - accuracy: 0.2053 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.8274 - accuracy: 0.2337 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.7367 - accuracy: 0.3598 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.5013 - accuracy: 0.3699 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.3388 - accuracy: 0.3882 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.2427 - accuracy: 0.3821 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.2158 - accuracy: 0.4573 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.3139 - accuracy: 0.4614 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.1110 - accuracy: 0.5285 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0989 - accuracy: 0.4512 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0701 - accuracy: 0.4797 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1222 - accuracy: 0.4980 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1747 - accuracy: 0.5528 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0443 - accuracy: 0.4980 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1087 - accuracy: 0.5772 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.1172 - accuracy: 0.5122 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.1181 - accuracy: 0.6057 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9544 - accuracy: 0.5041 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9979 - accuracy: 0.5813 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9267 - accuracy: 0.5935 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9501 - accuracy: 0.5467 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.9097 - accuracy: 0.5671 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.9588 - accuracy: 0.5427 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8936 - accuracy: 0.5813 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9967 - accuracy: 0.5467 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9360 - accuracy: 0.6159 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9190 - accuracy: 0.5630 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9426 - accuracy: 0.5833 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9326 - accuracy: 0.5874 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.0410 - accuracy: 0.5813 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0421 - accuracy: 0.5305 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9682 - accuracy: 0.5915 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9417 - accuracy: 0.5142 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0326 - accuracy: 0.5569 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9189 - accuracy: 0.5711 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9041 - accuracy: 0.5488 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9574 - accuracy: 0.5772 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9224 - accuracy: 0.5630 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.7968 - accuracy: 0.6037 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9072 - accuracy: 0.6037 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9073 - accuracy: 0.6118 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.8436 - accuracy: 0.6179 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9694 - accuracy: 0.5671 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.9404 - accuracy: 0.6280 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8817 - accuracy: 0.5569 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8579 - accuracy: 0.6402 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8508 - accuracy: 0.6199 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8464 - accuracy: 0.5854 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8440 - accuracy: 0.6443 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.9153 - accuracy: 0.6199 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d8e743c70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cn.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model_cn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   Y          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 5,791,052\n",
      "Non-trainable params: 42,023\n",
      "____________________________________________________________________________\n",
      "Fitting the end-to-end model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 16s 89ms/step - loss: 0.9072 - accuracy: 0.5407 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 1.0511 - accuracy: 0.5244 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.0015 - accuracy: 0.5386 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 1.3511 - accuracy: 0.4065 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.9494 - accuracy: 0.5000 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 1.1537 - accuracy: 0.4858 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.9557 - accuracy: 0.5569 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.6678 - accuracy: 0.4106 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.2895 - accuracy: 0.5102 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.0366 - accuracy: 0.4593 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.1873 - accuracy: 0.3821 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.8548 - accuracy: 0.5610 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 1.1804 - accuracy: 0.5386 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 1.0138 - accuracy: 0.5142 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.9223 - accuracy: 0.5915 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.7707 - accuracy: 0.6077 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.6013 - accuracy: 0.7236 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5853 - accuracy: 0.7378 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.6308 - accuracy: 0.7033 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.4605 - accuracy: 0.7480 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.4304 - accuracy: 0.8272 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.3837 - accuracy: 0.7988 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2913 - accuracy: 0.8618 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.3052 - accuracy: 0.8374 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2781 - accuracy: 0.8394 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2319 - accuracy: 0.8780 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.2501 - accuracy: 0.8618 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.1933 - accuracy: 0.8882 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.1649 - accuracy: 0.9268 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2969 - accuracy: 0.8699 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.5232 - accuracy: 0.8049 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.4445 - accuracy: 0.7907 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.3708 - accuracy: 0.8008 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.3562 - accuracy: 0.8455 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.2419 - accuracy: 0.8821 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1076 - accuracy: 0.9451 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1264 - accuracy: 0.9492 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0547 - accuracy: 0.9756 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0280 - accuracy: 0.9919 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0348 - accuracy: 0.9898 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0162 - accuracy: 0.9939 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0200 - accuracy: 0.9919 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.0173 - accuracy: 0.9919 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0148 - accuracy: 0.9919 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0285 - accuracy: 0.9939 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0144 - accuracy: 0.9919 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0387 - accuracy: 0.9898 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0354 - accuracy: 0.9858 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0566 - accuracy: 0.9756 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1003 - accuracy: 0.9654 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2da01aa5f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model_cn.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model_cn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation color normalization\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.9113 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.9112510681152344, 0.5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test dataset evaluation color normalization\")\n",
    "model_cn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5833\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.3750\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 epochs, tensor X,  0.41\n",
    "# 50 epochs, tensor X,  0.50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
