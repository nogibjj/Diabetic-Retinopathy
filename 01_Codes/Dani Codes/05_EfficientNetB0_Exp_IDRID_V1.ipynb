{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def restart_kernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)\n",
    "\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 18:14:11.991591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 18:14:15.074275: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-14 18:14:19.988602: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 18:14:19.988836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 18:14:19.988857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-14 18:14:25.913173: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-04-14 18:14:25.913238: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: codespaces-887f93\n",
      "2024-04-14 18:14:25.913252: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: codespaces-887f93\n",
      "2024-04-14 18:14:25.913442: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 545.23.8\n",
      "2024-04-14 18:14:25.913477: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 545.23.8\n",
      "2024-04-14 18:14:25.913489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 545.23.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable for TensorFlow\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth before initializing GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Handle potential errors here\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Set seeds to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize base model with pre-trained weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model RAW IDRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 18:14:37.789000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load the data\n",
    "\n",
    "X_train = np.load(\"../../aws_s3/Idrid_224_Raw/X_train.npy\")\n",
    "y_train = np.load(\"../../aws_s3/Idrid_224_Raw/y_train.npy\")\n",
    "X_test = np.load(\"../../aws_s3/Idrid_224_Raw/X_test.npy\")\n",
    "y_test = np.load(\"../../aws_s3/Idrid_224_Raw/y_test.npy\")\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float32\")\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=\"float32\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float32\")\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a new model on top with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freesze the base model\n",
    "base_model.trainable = False\n",
    "num_classes = 4\n",
    "epochs = 50\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# create class weight\n",
    "def compute_class_weights(labels):\n",
    "    # Convert labels to a 1D tensor if not already\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    # Get unique classes and their indices and counts\n",
    "    unique_classes, _, class_counts = tf.unique_with_counts(labels)\n",
    "\n",
    "    # Compute total number of samples\n",
    "    total_samples = tf.reduce_sum(class_counts)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = total_samples / (len(unique_classes) * class_counts)\n",
    "\n",
    "    # Create a class weights dictionary mapping class indices to their respective weights\n",
    "    class_weight_dict = dict(zip(unique_classes.numpy(), class_weights.numpy()))\n",
    "\n",
    "    return class_weight_dict\n",
    "\n",
    "\n",
    "class_weight_dict = compute_class_weights(y_train)\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 14s 450ms/step - loss: 1.8416 - accuracy: 0.2114 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 7s 455ms/step - loss: 1.6140 - accuracy: 0.2602 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 1.6353 - accuracy: 0.3516 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 7s 451ms/step - loss: 1.4381 - accuracy: 0.3516 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 8s 482ms/step - loss: 1.3216 - accuracy: 0.4492 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 7s 443ms/step - loss: 1.4094 - accuracy: 0.4268 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 7s 446ms/step - loss: 1.2302 - accuracy: 0.4268 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 1.2929 - accuracy: 0.4614 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 7s 450ms/step - loss: 1.1215 - accuracy: 0.5163 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 7s 443ms/step - loss: 1.0529 - accuracy: 0.5244 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 7s 445ms/step - loss: 1.1358 - accuracy: 0.4675 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 7s 445ms/step - loss: 1.0627 - accuracy: 0.4939 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 7s 437ms/step - loss: 1.1428 - accuracy: 0.6077 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 7s 441ms/step - loss: 0.9883 - accuracy: 0.5041 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 7s 444ms/step - loss: 1.0581 - accuracy: 0.6159 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 7s 443ms/step - loss: 1.0196 - accuracy: 0.4959 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 7s 441ms/step - loss: 1.1491 - accuracy: 0.6362 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 7s 448ms/step - loss: 0.9652 - accuracy: 0.5305 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 7s 446ms/step - loss: 0.8996 - accuracy: 0.6057 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.8345 - accuracy: 0.5996 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 7s 446ms/step - loss: 0.9335 - accuracy: 0.6260 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 7s 443ms/step - loss: 0.8613 - accuracy: 0.5915 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.9583 - accuracy: 0.5610 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 7s 432ms/step - loss: 0.8435 - accuracy: 0.5711 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 7s 433ms/step - loss: 0.8518 - accuracy: 0.5671 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 1.0546 - accuracy: 0.6138 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.8856 - accuracy: 0.5346 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 7s 451ms/step - loss: 0.9810 - accuracy: 0.6240 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 7s 446ms/step - loss: 0.8344 - accuracy: 0.6463 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.9001 - accuracy: 0.5833 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 7s 451ms/step - loss: 1.0557 - accuracy: 0.5508 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 0.9340 - accuracy: 0.6118 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.9237 - accuracy: 0.5467 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 7s 439ms/step - loss: 1.1320 - accuracy: 0.5732 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 7s 440ms/step - loss: 0.8742 - accuracy: 0.6382 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 0.8692 - accuracy: 0.5976 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 7s 448ms/step - loss: 1.0350 - accuracy: 0.5976 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 7s 436ms/step - loss: 0.8290 - accuracy: 0.6016 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 7s 453ms/step - loss: 0.7940 - accuracy: 0.6240 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 7s 440ms/step - loss: 0.8341 - accuracy: 0.6321 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 7s 446ms/step - loss: 0.9948 - accuracy: 0.6118 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 7s 441ms/step - loss: 0.9506 - accuracy: 0.6504 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 7s 434ms/step - loss: 0.9219 - accuracy: 0.5813 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 7s 441ms/step - loss: 0.8482 - accuracy: 0.6463 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.8870 - accuracy: 0.5915 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 7s 439ms/step - loss: 0.8838 - accuracy: 0.6280 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 7s 435ms/step - loss: 0.8858 - accuracy: 0.6159 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 0.9070 - accuracy: 0.5711 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 7s 441ms/step - loss: 0.8477 - accuracy: 0.6667 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 7s 436ms/step - loss: 0.8971 - accuracy: 0.5915 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f2d0405e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fine tuning: unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   Y          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 5,791,052\n",
      "Non-trainable params: 42,023\n",
      "____________________________________________________________________________\n",
      "Fitting the end-to-end model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 43s 2s/step - loss: 0.9230 - accuracy: 0.5955 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.9385 - accuracy: 0.5976 - lr: 0.0011\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.1054 - accuracy: 0.4898 - lr: 0.0021\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.1402 - accuracy: 0.5122 - lr: 0.0031\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0170 - accuracy: 0.5142 - lr: 0.0041\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.9252 - accuracy: 0.5874 - lr: 0.0050\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0920 - accuracy: 0.4797 - lr: 0.0060\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.2045 - accuracy: 0.5122 - lr: 0.0070\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 1.1007 - accuracy: 0.5224 - lr: 0.0080\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0369 - accuracy: 0.4959 - lr: 0.0090\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.2789 - accuracy: 0.4411 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.9274 - accuracy: 0.5630 - lr: 0.0090\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0657 - accuracy: 0.5305 - lr: 0.0080\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0204 - accuracy: 0.5163 - lr: 0.0070\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.0585 - accuracy: 0.6057 - lr: 0.0060\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.7630 - accuracy: 0.5813 - lr: 0.0050\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.7318 - accuracy: 0.6382 - lr: 0.0041\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.6883 - accuracy: 0.7134 - lr: 0.0031\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.6106 - accuracy: 0.6484 - lr: 0.0021\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.5460 - accuracy: 0.7663 - lr: 0.0011\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.4084 - accuracy: 0.7114 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.3506 - accuracy: 0.8028 - lr: 0.0011\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.3211 - accuracy: 0.8171 - lr: 0.0021\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.3150 - accuracy: 0.8252 - lr: 0.0031\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.3845 - accuracy: 0.8008 - lr: 0.0041\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.3372 - accuracy: 0.8191 - lr: 0.0050\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.2662 - accuracy: 0.8374 - lr: 0.0060\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.1917 - accuracy: 0.8923 - lr: 0.0070\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.1917 - accuracy: 0.9065 - lr: 0.0080\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.4308 - accuracy: 0.8252 - lr: 0.0090\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.5322 - accuracy: 0.8008 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.5051 - accuracy: 0.7744 - lr: 0.0090\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.3962 - accuracy: 0.8130 - lr: 0.0080\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.6724 - accuracy: 0.7195 - lr: 0.0070\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.3011 - accuracy: 0.8333 - lr: 0.0060\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.1472 - accuracy: 0.9309 - lr: 0.0050\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.1304 - accuracy: 0.9472 - lr: 0.0041\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0716 - accuracy: 0.9695 - lr: 0.0031\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0546 - accuracy: 0.9776 - lr: 0.0021\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0299 - accuracy: 0.9878 - lr: 0.0011\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.0360 - accuracy: 0.9878 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0339 - accuracy: 0.9898 - lr: 0.0011\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.0518 - accuracy: 0.9817 - lr: 0.0021\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0302 - accuracy: 0.9898 - lr: 0.0031\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0313 - accuracy: 0.9858 - lr: 0.0041\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0359 - accuracy: 0.9919 - lr: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0358 - accuracy: 0.9878 - lr: 0.0060\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0652 - accuracy: 0.9756 - lr: 0.0070\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.0758 - accuracy: 0.9634 - lr: 0.0080\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 28s 2s/step - loss: 0.1459 - accuracy: 0.9451 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f24362290>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.keras\")\n",
    "# 50 epochs == 41% accuracy\n",
    "# 25 epochs == 45% accuracy\n",
    "# 20 epochs == 45.83% accuracy\n",
    "# 25 epochs == 62.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.0285 - accuracy: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0284910202026367, 0.5833333134651184]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test dataset evaluation\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epcohs, tensor X, 0.4583\n",
    "# 20 epcohs, tensor X, 0.5000\n",
    "# 20 epcohs, tensor X, 0.5000\n",
    "# 20 epcohs, tensor X, 0.5417\n",
    "# 20 epcohs, tensor X, 0.4583\n",
    "# 20 epcohs, tensor X, 0.5833\n",
    "# 20 epcohs, tensor X  0.5417\n",
    "# 20 epcohs, tensor X  0.5417\n",
    "# 20 epcohs, tensor X  0.6667\n",
    "# 20 epcohs, tensor X. 0.5000\n",
    "# 20 epcohs, tensor X. 0.4583\n",
    "# 20 epcohs, tensor X.  0.5000\n",
    "# 20 epcohs, tensor X.  0.5417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 epochs, tensor X, 0.5833\n",
    "# 50 epochs, tensor X, 0.25\n",
    "# 50 epochs, tensor X, tensor Y, 0.58\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = np.load(\"../../aws_s3/Idrid_224_CN/X_train.npy\")\n",
    "y_train = np.load(\"../../aws_s3/Idrid_224_CN/y_train.npy\")\n",
    "X_test = np.load(\"../../aws_s3/Idrid_224_CN/X_test.npy\")\n",
    "y_test = np.load(\"../../aws_s3/Idrid_224_CN/y_test.npy\")\n",
    "\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=\"float32\")\n",
    "# y_train = tf.convert_to_tensor(y_train, dtype=\"float32\")\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=\"float32\")\n",
    "# y_test = tf.convert_to_tensor(y_test, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout_2 (Dropout)         (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense_3 (Dense)             (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_3 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_4 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_5 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freesze the base model\n",
    "base_model.trainable = False\n",
    "num_classes = 4\n",
    "epochs = 50\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model_cn = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_cn.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 14s 491ms/step - loss: 1.8780 - accuracy: 0.2053 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 8s 506ms/step - loss: 1.8274 - accuracy: 0.2337 - lr: 0.0011\n",
      "Epoch 3/50\n",
      " 9/16 [===============>..............] - ETA: 3s - loss: 1.3799 - accuracy: 0.3576"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model_cn\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the top layer of the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel_cn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_cn.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model_cn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model_cn.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model_cn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset evaluation color normalization\")\n",
    "model_cn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5833\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.3750\n",
    "# 20 epochs, tensor X 0.5000\n",
    "# 20 epochs, tensor X 0.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 epochs, tensor X,  0.41\n",
    "# 50 epochs, tensor X,  0.50\n",
    "# 50 epochs, tensor X, 0.541666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
