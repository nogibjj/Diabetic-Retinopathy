{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def restart_kernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)\n",
    "\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable for TensorFlow\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth before initializing GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Handle potential errors here\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Set seeds to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = np.load(\"../../aws_s3/messidor_224/X_train.npy\")\n",
    "y_train = np.load(\"../../aws_s3/messidor_224/y_train.npy\")\n",
    "X_test = np.load(\"../../aws_s3/messidor_224/X_test.npy\")\n",
    "y_test = np.load(\"../../aws_s3/messidor_224/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a new model on top with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout_2 (Dropout)         (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense_3 (Dense)             (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_3 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_4 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_5 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "epochs = 50\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "base_learning_rate = 1e-1\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "\n",
    "# creating a learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        return lr\n",
    "    elif epoch < 24:\n",
    "        return lr - 0.01\n",
    "    elif epoch < 35:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# Function to compute class weights using TensorFlow\n",
    "def compute_class_weights(labels):\n",
    "    # Convert labels to a 1D tensor if not already\n",
    "    labels = tf.reshape(labels, [-1])\n",
    "\n",
    "    # Get unique classes and their indices and counts\n",
    "    unique_classes, _, class_counts = tf.unique_with_counts(labels)\n",
    "\n",
    "    # Compute total number of samples\n",
    "    total_samples = tf.reduce_sum(class_counts)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = total_samples / (len(unique_classes) * class_counts)\n",
    "\n",
    "    # Create a class weights dictionary mapping class indices to their respective weights\n",
    "    class_weight_dict = dict(zip(unique_classes.numpy(), class_weights.numpy()))\n",
    "\n",
    "    return class_weight_dict\n",
    "\n",
    "\n",
    "class_weight_dict = compute_class_weights(y_train)\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1163, 224, 224, 3]),\n",
       " TensorShape([1163]),\n",
       " TensorShape([24, 224, 224, 3]),\n",
       " TensorShape([24]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 23s 457ms/step - loss: 2.5894 - accuracy: 0.3138 - lr: 0.1000\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 17s 451ms/step - loss: 2.8601 - accuracy: 0.3044 - lr: 0.1000\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 2.7349 - accuracy: 0.3207 - lr: 0.1000\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 3.0149 - accuracy: 0.3113 - lr: 0.1000\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 2.7447 - accuracy: 0.3233 - lr: 0.1000\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 3.0540 - accuracy: 0.3061 - lr: 0.1000\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 17s 454ms/step - loss: 2.6421 - accuracy: 0.3302 - lr: 0.1000\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 17s 452ms/step - loss: 3.1791 - accuracy: 0.3216 - lr: 0.1000\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 17s 446ms/step - loss: 3.0804 - accuracy: 0.3293 - lr: 0.1000\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 3.0679 - accuracy: 0.3474 - lr: 0.1000\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 17s 452ms/step - loss: 3.0362 - accuracy: 0.3233 - lr: 0.1000\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 17s 453ms/step - loss: 3.2418 - accuracy: 0.3293 - lr: 0.1000\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 3.1377 - accuracy: 0.3345 - lr: 0.1000\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 17s 454ms/step - loss: 2.7046 - accuracy: 0.3422 - lr: 0.1000\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 3.5246 - accuracy: 0.3164 - lr: 0.1000\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 2.8917 - accuracy: 0.3508 - lr: 0.0900\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 16s 446ms/step - loss: 2.6193 - accuracy: 0.3560 - lr: 0.0800\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 17s 447ms/step - loss: 2.3991 - accuracy: 0.3448 - lr: 0.0700\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 17s 448ms/step - loss: 2.3233 - accuracy: 0.3938 - lr: 0.0600\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 17s 451ms/step - loss: 2.0278 - accuracy: 0.3689 - lr: 0.0500\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 17s 448ms/step - loss: 1.9396 - accuracy: 0.3603 - lr: 0.0400\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 16s 445ms/step - loss: 1.6858 - accuracy: 0.3998 - lr: 0.0300\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 17s 452ms/step - loss: 1.4688 - accuracy: 0.4230 - lr: 0.0200\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 17s 451ms/step - loss: 1.4082 - accuracy: 0.3826 - lr: 0.0100\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 17s 456ms/step - loss: 1.2702 - accuracy: 0.4058 - lr: 0.0100\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 17s 455ms/step - loss: 1.2318 - accuracy: 0.4531 - lr: 0.0100\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 1.2330 - accuracy: 0.4764 - lr: 0.0100\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 1.2286 - accuracy: 0.4488 - lr: 0.0100\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 1.2647 - accuracy: 0.4291 - lr: 0.0100\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 17s 452ms/step - loss: 1.2172 - accuracy: 0.4669 - lr: 0.0100\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 17s 451ms/step - loss: 1.2710 - accuracy: 0.4076 - lr: 0.0100\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 17s 454ms/step - loss: 1.2411 - accuracy: 0.4635 - lr: 0.0100\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 1.1945 - accuracy: 0.4772 - lr: 0.0100\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 17s 448ms/step - loss: 1.2419 - accuracy: 0.4514 - lr: 0.0100\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 17s 449ms/step - loss: 1.2521 - accuracy: 0.4342 - lr: 0.0100\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 17s 453ms/step - loss: 1.2320 - accuracy: 0.4480 - lr: 0.0090\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 16s 445ms/step - loss: 1.2145 - accuracy: 0.4789 - lr: 0.0082\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 1.2229 - accuracy: 0.4497 - lr: 0.0074\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 1.1781 - accuracy: 0.4764 - lr: 0.0067\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 16s 444ms/step - loss: 1.1648 - accuracy: 0.4574 - lr: 0.0061\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 16s 445ms/step - loss: 1.1938 - accuracy: 0.4686 - lr: 0.0055\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 17s 448ms/step - loss: 1.1582 - accuracy: 0.4832 - lr: 0.0050\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 17s 447ms/step - loss: 1.1562 - accuracy: 0.4893 - lr: 0.0045\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 16s 445ms/step - loss: 1.1466 - accuracy: 0.4987 - lr: 0.0041\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 16s 443ms/step - loss: 1.1561 - accuracy: 0.4953 - lr: 0.0037\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 17s 450ms/step - loss: 1.1395 - accuracy: 0.4944 - lr: 0.0033\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 17s 452ms/step - loss: 1.1538 - accuracy: 0.4609 - lr: 0.0030\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 16s 444ms/step - loss: 1.1262 - accuracy: 0.4807 - lr: 0.0027\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 17s 448ms/step - loss: 1.1319 - accuracy: 0.4815 - lr: 0.0025\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 16s 443ms/step - loss: 1.1453 - accuracy: 0.4901 - lr: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f300703f3d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fine tuning: unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   Y          \n",
      "                                                                            \n",
      " dropout_2 (Dropout)         (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense_3 (Dense)             (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_3 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_4 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_5 (Dense)             (None, 4)                 2004      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,075\n",
      "Trainable params: 5,791,052\n",
      "Non-trainable params: 42,023\n",
      "____________________________________________________________________________\n",
      "Fitting the end-to-end model\n",
      "Epoch 1/50\n",
      "37/37 [==============================] - 78s 2s/step - loss: 39762.6211 - accuracy: 0.2296 - lr: 0.1000\n",
      "Epoch 2/50\n",
      "28/37 [=====================>........] - ETA: 16s - loss: 1.5426 - accuracy: 0.2087"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 2.9535 - accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.953517198562622, 0.625]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test dataset evaluation\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
