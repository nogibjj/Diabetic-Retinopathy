{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def restart_kernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)\n",
    "\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:08:12.350008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 11:08:12.512266: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-12 11:08:13.315567: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-12 11:08:13.315688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-12 11:08:13.315706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:08:14.536913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 11:08:14.821725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14485 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable for TensorFlow\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth before initializing GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Handle potential errors here\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Set seeds to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"../../aws_s3/DDR+APTOS_TRAIN_TEST/train_test_DDR_APTOS.csv\")\n",
    "\n",
    "filtered_data = data[data[\"Data_source\"] == \"DDR\"]\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = filtered_data[filtered_data[\"Split\"] == \"Train\"]\n",
    "test_data = filtered_data[filtered_data[\"Split\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directories\n",
    "luminosity_output_dir_train = \"luminosity_images_train/\"\n",
    "cropped_output_dir_train = \"cropped_images_train/\"\n",
    "normalized_output_dir_train = \"normalized_images_train/\"\n",
    "augmentation_output_dir_train = \"augmented_images_train/\"\n",
    "luminosity_output_dir_test = \"luminosity_images_test/\"\n",
    "cropped_output_dir_test = \"cropped_images_test/\"\n",
    "normalized_output_dir_test = \"normalized_images_test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(luminosity_output_dir_train, exist_ok=True)\n",
    "os.makedirs(cropped_output_dir_train, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_train, exist_ok=True)\n",
    "os.makedirs(augmentation_output_dir_train, exist_ok=True)\n",
    "os.makedirs(luminosity_output_dir_test, exist_ok=True)\n",
    "os.makedirs(cropped_output_dir_test, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luminosity and noise removal function\n",
    "def luminosity_image(image_path, output_dir):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    l_eq = clahe.apply(l)\n",
    "\n",
    "    # Merge back LAB channels\n",
    "    lab_eq = cv2.merge((l_eq, a, b))\n",
    "    enhanced_image = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Image noise removal using Gaussian filter\n",
    "    filtered_image = cv2.GaussianBlur(enhanced_image, (5, 5), 0)\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, threshold=15, resize_flag=False, desired_size=(224, 224)):\n",
    "    img_np = np.array(img)\n",
    "    x_dim = img_np.shape[0]\n",
    "    y_dim = img_np.shape[1]\n",
    "    pixel_sums = img_np.sum(axis=2)\n",
    "    x_arr = pixel_sums.sum(axis=1)\n",
    "    y_arr = pixel_sums.sum(axis=0)\n",
    "    x_start = np.where(x_arr > threshold * y_dim)[0][0]\n",
    "    x_end = np.where(x_arr > threshold * y_dim)[0][-1]\n",
    "    y_start = np.where(y_arr > threshold * x_dim)[0][0]\n",
    "    y_end = np.where(y_arr > threshold * x_dim)[0][-1]\n",
    "    new_img = img_np[x_start:x_end, y_start:y_end]\n",
    "    new_img = Image.fromarray(new_img)\n",
    "    if resize_flag:\n",
    "        new_img = new_img.resize(desired_size)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img):\n",
    "    # Convert image to numpy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Calculate mean and standard deviation (std) channel-wise\n",
    "    mean_channels = np.mean(img_np, axis=(0, 1))\n",
    "    std_channels = np.std(img_np, axis=(0, 1))\n",
    "\n",
    "    # Normalize each channel separately\n",
    "    normalized_image = np.zeros_like(img_np, dtype=np.float32)\n",
    "    for channel in range(img_np.shape[2]):\n",
    "        normalized_image[:, :, channel] = (\n",
    "            img_np[:, :, channel] - mean_channels[channel]\n",
    "        ) / std_channels[channel]\n",
    "\n",
    "    # Scale values to be within [0, 255]\n",
    "    normalized_image = (\n",
    "        (normalized_image - np.min(normalized_image))\n",
    "        / (np.max(normalized_image) - np.min(normalized_image))\n",
    "        * 255\n",
    "    )\n",
    "\n",
    "    # Clip and return the normalized image\n",
    "    normalized_image = np.clip(normalized_image, 0, 255)\n",
    "    return normalized_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## image is an np array of the image\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    height, width = image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def flip_image(image):\n",
    "    flip_variations = [\n",
    "        (False, False),  # No flip\n",
    "        (True, False),  # Horizontal flip\n",
    "        (False, True),  # Vertical flip\n",
    "        (True, True),  # Both flips\n",
    "    ]\n",
    "\n",
    "    random.shuffle(flip_variations)\n",
    "    augmented_images = []\n",
    "\n",
    "    for flip_horizontal, flip_vertical in flip_variations[:4]:\n",
    "        if flip_horizontal and flip_vertical:\n",
    "            augmented_images.append(cv2.flip(image, -1))  # horizontal and vertical\n",
    "        elif flip_horizontal:\n",
    "            augmented_images.append(cv2.flip(image, 1))  # horizontal\n",
    "        elif flip_vertical:\n",
    "            augmented_images.append(cv2.flip(image, 0))  # vertical\n",
    "        else:\n",
    "            augmented_images.append(image)  # No flip\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "def shear_image(image, shear_range):\n",
    "    height, width = image.shape[:2]\n",
    "    shear_value = random.uniform(-shear_range, shear_range)\n",
    "\n",
    "    if shear_value < 0:\n",
    "        shear_matrix = np.array([[1, -shear_value, 0], [0, 1, 0]])\n",
    "    else:\n",
    "        shear_matrix = np.array([[1, shear_value, 0], [0, 1, 0]])\n",
    "\n",
    "    sheared_image = cv2.warpAffine(image, shear_matrix, (width, height))\n",
    "    return sheared_image\n",
    "\n",
    "\n",
    "def translate_image(image, translate_range):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    max_shift_x = int(width * 0.1)\n",
    "    max_shift_y = int(height * 0.1)\n",
    "\n",
    "    translate_x = random.randint(-max_shift_x, max_shift_x)\n",
    "    translate_y = random.randint(-max_shift_y, max_shift_y)\n",
    "\n",
    "    translation_matrix = np.array(\n",
    "        [[1, 0, translate_x], [0, 1, translate_y]], dtype=np.float32\n",
    "    )\n",
    "\n",
    "    translated_image = cv2.warpAffine(image, translation_matrix, (width, height))\n",
    "\n",
    "    return translated_image\n",
    "\n",
    "\n",
    "def adjust_brightness(image, brightness_range):\n",
    "    brightness_factor = 1.0 + random.uniform(-brightness_range, brightness_range)\n",
    "    adjusted_image = np.clip(image * brightness_factor, 0.25, 255).astype(np.uint8)\n",
    "    return adjusted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## image parameter is an np array of an image\n",
    "## flag 1: includes brightness, 0 or any other value exludes it\n",
    "\n",
    "\n",
    "def augmented_fn2(image, flag):\n",
    "    augmented_images = []\n",
    "\n",
    "    if flag == 1:\n",
    "        for _ in range(4):\n",
    "            angle = random.uniform(-35, 35)\n",
    "            image_r = rotate_image(image, angle)\n",
    "            augmented_images.append(image_r)\n",
    "\n",
    "        image_f = flip_image(image)\n",
    "        augmented_images.extend(image_f)\n",
    "\n",
    "        for _ in range(4):\n",
    "            image_s = shear_image(image, shear_range=0.15)\n",
    "            augmented_images.append(image_s)\n",
    "\n",
    "        for _ in range(4):\n",
    "            image_t = translate_image(image, translate_range=0.1)\n",
    "            augmented_images.append(image_t)\n",
    "\n",
    "        for _ in range(4):\n",
    "            image_b = adjust_brightness(image, brightness_range=0.25)\n",
    "            augmented_images.append(image_b)\n",
    "    else:\n",
    "        for _ in range(4):\n",
    "            angle = random.uniform(-35, 35)\n",
    "            image_r = rotate_image(image, angle)\n",
    "            augmented_images.append(image_r)\n",
    "\n",
    "        image_f = flip_image(image)\n",
    "        augmented_images.extend(image_f)\n",
    "\n",
    "        for _ in range(4):\n",
    "            image_s = shear_image(image, shear_range=0.15)\n",
    "            augmented_images.append(image_s)\n",
    "\n",
    "        for _ in range(4):\n",
    "            image_t = translate_image(image, translate_range=0.1)\n",
    "            augmented_images.append(image_t)\n",
    "\n",
    "    return np.array(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 34 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 35 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 38 extraneous bytes before marker 0xd9\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "# Function to get the correct image path with extension\n",
    "def get_image_path(base_dir, img_name):\n",
    "    for ext in [\"jpg\", \"png\"]:\n",
    "        if \"jpg\" in img_name or \"png\" in img_name:\n",
    "            temp_path = f\"{base_dir}/{img_name}\"\n",
    "        else:\n",
    "            temp_path = f\"{base_dir}/{img_name}.{ext}\"\n",
    "        if os.path.exists(temp_path):\n",
    "            return temp_path\n",
    "    return None  # Return None if no file is found\n",
    "\n",
    "\n",
    "# Load and preprocess each image\n",
    "for index, row in train_data.iterrows():\n",
    "    img_name, label = row[\"Image_ID\"], row[\"Retinopathy_Grade\"]\n",
    "    image_path = get_image_path(\n",
    "        \"../../aws_s3/DDR+APTOS_TRAIN_TEST/train_new\", img_name\n",
    "    )  # Dynamically get the correct image path\n",
    "\n",
    "    if image_path is None:\n",
    "        print(f\"Image file for {img_name} not found in JPG or PNG format.\")\n",
    "        continue  # Skip this iteration if the file doesn't exist\n",
    "\n",
    "    # Change Luminosity and do noise removal for the image\n",
    "    luminosity_img_path = luminosity_image(image_path, luminosity_output_dir_train)\n",
    "\n",
    "    # Crop and resize the image\n",
    "    luminosity_img = Image.open(luminosity_img_path)\n",
    "    cropped_resized_img = crop_image(\n",
    "        luminosity_img, resize_flag=True, desired_size=(224, 224)\n",
    "    )\n",
    "\n",
    "    # Save cropped image\n",
    "    cropped_img_name = f\"{img_name}_{index+1}.jpg\"\n",
    "    cropped_img_path = os.path.join(cropped_output_dir_train, cropped_img_name)\n",
    "    cropped_resized_img.save(cropped_img_path)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_img = normalize_image(cropped_resized_img)\n",
    "\n",
    "    # Save normalized image\n",
    "    normalized_img_name = f\"norm_{img_name}_{index+1}.jpg\"\n",
    "    normalized_img_path = os.path.join(normalized_output_dir_train, normalized_img_name)\n",
    "    cv2.imwrite(normalized_img_path, cv2.cvtColor(normalized_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Augment image 20 times\n",
    "    augmented_images = augmented_fn2(normalized_img, flag=1)\n",
    "\n",
    "    for aug_index, augmented_img in enumerate(augmented_images):\n",
    "        # Use aug_index to create a unique filename for each augmented image\n",
    "        augmented_img_path = os.path.join(\n",
    "            augmentation_output_dir_train, f\"{img_name}_{index+1}_{aug_index+1}.jpg\"\n",
    "        )\n",
    "        cv2.imwrite(augmented_img_path, cv2.cvtColor(augmented_img, cv2.COLOR_RGB2BGR))\n",
    "        # Append to X_train and y_train\n",
    "        X_train.append(augmented_img)\n",
    "        y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Load and preprocess each image\n",
    "for index, row in test_data.iterrows():\n",
    "    img_name, label = row[\"Image_ID\"], row[\"Retinopathy_Grade\"]\n",
    "    image_path = get_image_path(\n",
    "        \"../../aws_s3/DDR+APTOS_TRAIN_TEST/test_new\", img_name\n",
    "    )  # Dynamically get the correct image path\n",
    "\n",
    "    if image_path is None:\n",
    "        print(f\"Image file for {img_name} not found in JPG or PNG format.\")\n",
    "        continue  # Skip this iteration if the file doesn't exist\n",
    "\n",
    "    # Preprocess the image\n",
    "    luminosity_img_path = luminosity_image(image_path, luminosity_output_dir_test)\n",
    "\n",
    "    # Crop and resize the image\n",
    "    luminosity_img = Image.open(luminosity_img_path)\n",
    "    cropped_resized_img = crop_image(\n",
    "        luminosity_img, resize_flag=True, desired_size=(224, 224)\n",
    "    )\n",
    "\n",
    "    # Save cropped image\n",
    "    cropped_img_name = f\"{img_name}_{index+1}.jpg\"\n",
    "    cropped_img_path = os.path.join(cropped_output_dir_test, cropped_img_name)\n",
    "    cropped_resized_img.save(cropped_img_path)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_img = normalize_image(cropped_resized_img)\n",
    "\n",
    "    # Save normalized image\n",
    "    normalized_img_name = f\"norm_{img_name}_{index+1}.jpg\"\n",
    "    normalized_img_path = os.path.join(normalized_output_dir_test, normalized_img_name)\n",
    "    cv2.imwrite(normalized_img_path, cv2.cvtColor(normalized_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Append to X_test and y_test\n",
    "    X_test.append(normalized_img)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these as pickle files so that we can use them later in case of any issues\n",
    "np.save(\"X_train_aug.npy\", X_train)\n",
    "np.save(\"y_train_aug.npy\", y_train)\n",
    "np.save(\"X_test_aug.npy\", X_test)\n",
    "np.save(\"y_test_aug.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize base model with pre-trained weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = np.load(\"X_train_aug.npy\")\n",
    "y_train = np.load(\"y_train_aug.npy\")\n",
    "X_test = np.load(\"X_test_aug.npy\")\n",
    "y_test = np.load(\"y_test_aug.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a new model on top with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   N          \n",
      "                                                                            \n",
      " dropout (Dropout)           (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense (Dense)               (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_1 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             (None, 5)                 2505      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,576\n",
      "Trainable params: 1,784,005\n",
      "Non-trainable params: 4,049,571\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5\n",
    "epochs = 10\n",
    "# Define the inputs\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Ensure the base_model is running in inference mode.\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)  # Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Dropout(0.5)(x)  # Another Dropout layer with 50% dropout rate\n",
    "# Adding FC (Fully Connected) layers\n",
    "x = layers.Dense(500)(x)\n",
    "\n",
    "# Adding a final layer with SoftMax activation for classification\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Creating the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate\n",
    "max_learning_rate = 1e-2\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200380, 224, 224, 3), (200380,), (2503, 224, 224, 3), (2503,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the top layer of the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:13:25.092533: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15052800000 exceeds 10% of free system memory.\n",
      "2024-04-12 11:13:34.119454: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15052800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:13:47.917357: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 22.12MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:47.917424: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n",
      "2024-04-12 11:13:48.335680: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2024-04-12 11:13:49.913887: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.08MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925324: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.08MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925366: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925386: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925402: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 21.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925417: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 21.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925431: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.925445: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-12 11:13:49.948575: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:1134 : NOT_FOUND: No algorithm worked!  Error messages:\n",
      "  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n",
      "  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n",
      "  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n",
      "  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n",
      "  Profiling failure on CUDNN engine 2#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n",
      "  Profiling failure on CUDNN engine 2: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n",
      "  Profiling failure on CUDNN engine 5#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n",
      "  Profiling failure on CUDNN engine 5: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/efficientnetb0/stem_conv/Conv2D' defined at (most recent call last):\n    File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2557/742491035.py\", line 6, in <module>\n      model.fit(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/efficientnetb0/stem_conv/Conv2D'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 2#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n  Profiling failure on CUDNN engine 2: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n  Profiling failure on CUDNN engine 5#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n  Profiling failure on CUDNN engine 5: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n\t [[{{node model/efficientnetb0/stem_conv/Conv2D}}]] [Op:__inference_train_function_14573]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Diabetic-Retinopathy/01_Codes/Tina Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb Cell 28\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFitting the top layer of the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     X_train[:\u001b[39m100000\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     y_train[:\u001b[39m100000\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight_dict,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lr_scheduler],\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-computing-machine-v6vwj7x5xpjvfpvxg/workspaces/Diabetic-Retinopathy/01_Codes/Tina%20Codes/EfficientNetB0_Aug_preprocessed_DDR.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'model/efficientnetb0/stem_conv/Conv2D' defined at (most recent call last):\n    File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2557/742491035.py\", line 6, in <module>\n      model.fit(\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/vscode/.local/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/efficientnetb0/stem_conv/Conv2D'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16857088 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 2#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n  Profiling failure on CUDNN engine 2: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 22196224 bytes.\n  Profiling failure on CUDNN engine 5#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n  Profiling failure on CUDNN engine 5: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 18413568 bytes.\n\t [[{{node model/efficientnetb0/stem_conv/Conv2D}}]] [Op:__inference_train_function_14573]"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the top layer of the model\")\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=4,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fine tuning: unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         Y          \n",
      "                                                                            \n",
      " efficientnetb0 (Functional)  (None, 1280)             4049571   Y          \n",
      "                                                                            \n",
      " dropout_4 (Dropout)         (None, 1280)              0         Y          \n",
      "                                                                            \n",
      " dense_6 (Dense)             (None, 1000)              1281000   Y          \n",
      "                                                                            \n",
      " dropout_5 (Dropout)         (None, 1000)              0         Y          \n",
      "                                                                            \n",
      " dense_7 (Dense)             (None, 500)               500500    Y          \n",
      "                                                                            \n",
      " dense_8 (Dense)             (None, 5)                 2505      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 5,833,576\n",
      "Trainable params: 5,791,553\n",
      "Non-trainable params: 42,023\n",
      "____________________________________________________________________________\n",
      "Fitting the end-to-end model\n",
      "Epoch 1/10\n",
      "314/314 [==============================] - 41s 86ms/step - loss: 1.3690 - accuracy: 0.5139 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.4294 - accuracy: 0.4622 - lr: 0.0011\n",
      "Epoch 3/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.3761 - accuracy: 0.4809 - lr: 0.0021\n",
      "Epoch 4/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.2709 - accuracy: 0.4868 - lr: 0.0031\n",
      "Epoch 5/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.2573 - accuracy: 0.5061 - lr: 0.0041\n",
      "Epoch 6/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.2810 - accuracy: 0.4796 - lr: 0.0050\n",
      "Epoch 7/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.3000 - accuracy: 0.4855 - lr: 0.0060\n",
      "Epoch 8/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.4025 - accuracy: 0.4668 - lr: 0.0070\n",
      "Epoch 9/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.4693 - accuracy: 0.4583 - lr: 0.0080\n",
      "Epoch 10/10\n",
      "314/314 [==============================] - 27s 86ms/step - loss: 1.4646 - accuracy: 0.4088 - lr: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec86054d00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "# Re-instantiate the optimizer\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9, clipnorm=1.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Fitting the end-to-end model\")\n",
    "# Train end-to-end. Stop before overfit\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=4,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset evaluation\n",
      "79/79 [==============================] - 3s 22ms/step - loss: 0.9907 - accuracy: 0.6221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9907349348068237, 0.6220535635948181]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test dataset evaluation\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
