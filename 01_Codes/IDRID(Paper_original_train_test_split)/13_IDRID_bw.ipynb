{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDRID with Dropout-Black and White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"test_split_image_key.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split_type\"] == \"training\"]\n",
    "test_data = data[data[\"split_type\"] == \"testing\"]\n",
    "\n",
    "# Define output directories for preprocessed images\n",
    "output_dir_train = \"preprocessed_images_bw/train/\"\n",
    "output_dir_test = \"preprocessed_images_bw/test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir_train, exist_ok=True)\n",
    "os.makedirs(output_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, output_dir):\n",
    "    # Load and resize the image to 512x512\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (512, 512))\n",
    "\n",
    "    # Convert image to grayscale (black and white)\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Optionally, you can still apply Gaussian filter to the grayscale image to remove noise\n",
    "    filtered_image = gaussian_filter(gray_image, sigma=1)\n",
    "\n",
    "    # Save the preprocessed grayscale image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "\n",
    "# Apply preprocessing to training and testing images\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"a_training/\" + img_name + \".jpg\"\n",
    "    preprocess_image(image_path, output_dir_train)\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"b_testing/\" + img_name + \".jpg\"\n",
    "    preprocess_image(image_path, output_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to numpy arrays\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = os.path.join(output_dir_train, img_name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    X_train.append(image)\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = os.path.join(output_dir_test, img_name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    X_test.append(image)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Extract labels directly without one-hot encoding\n",
    "y_train = train_data[\"Retinopathy grade\"].values\n",
    "y_test = test_data[\"Retinopathy grade\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 512, 3)\n",
    "num_classes = 5\n",
    "epochs = 10  # placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn512_dropout = models.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        # Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2)),\n",
    "        # Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3)),\n",
    "        # Layer 2\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3)),\n",
    "        # Layer 6\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3)),\n",
    "        # Layer 10\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3)),\n",
    "        # Layer 14\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "        layers.Conv2D(128, kernel_size=(3, 3)),\n",
    "        # Layer 18\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 19\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 20\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "        layers.Conv2D(200, kernel_size=(3, 3)),\n",
    "        # Layer 22\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 24\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 25\tFlatten layer\t-\n",
    "        layers.Flatten(),\n",
    "        # Layer 26\tFC layer\tNeurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 27\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 28\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 29\tFC layer\tNeurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 30\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 31\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        # Layer 32\tFC layer with Softmax activatio\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn512_dropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "history_cnn512_dropout = cnn512_dropout.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = cnn512_dropout.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
