{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CNN512 + no dropout + hyperparameters tuning + luminocity(image enhancement + noise removal) + color normalization + data augmentation using IDRID original train test split (following the paper's instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "luminocity\n",
    "\n",
    "Image Enhancement: Two methods were used to enhance the images, the enhance luminosity method [54] and Contrast Limited Adaptive Histogram Equalization (CLAHE). CLAHE [55] is successful in enhancing the contrast of the fundus images [56] and improving the low contrast of medical images [57]. CLAHE is applied to the L channel of the retina images that have a higher contrast [44], with tile size 8 × 8 and Clip Limit 5.0.\n",
    "\n",
    "Image Noise Removal: The CLAHE method can cause some noise in the images [54] and, to remove this noise, we applied the Gaussian filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"test_split_image_key.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split_type\"] == \"training\"]\n",
    "test_data = data[data[\"split_type\"] == \"testing\"]\n",
    "\n",
    "# Define output directories for preprocessed images\n",
    "output_dir_train = \"preprocessed_images_luminosity_new/train/\"\n",
    "output_dir_test = \"preprocessed_images_luminosity_new/test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir_train, exist_ok=True)\n",
    "os.makedirs(output_dir_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image_path, output_dir):\n",
    "    # Load and resize the image to 512x512\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (512, 512))\n",
    "\n",
    "    # Enhance luminosity and apply Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "    lab = cv2.cvtColor(resized_image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    l_eq = clahe.apply(l)\n",
    "    lab_eq = cv2.merge((l_eq, a, b))\n",
    "    enhanced_image = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Image noise removal using Gaussian filter\n",
    "    filtered_image = gaussian_filter(enhanced_image, sigma=1)\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "\n",
    "# Apply preprocessing to training and testing images\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"a_training/\" + img_name + \".jpg\"\n",
    "    preprocess_image(image_path, output_dir_train)\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"b_testing/\" + img_name + \".jpg\"\n",
    "    preprocess_image(image_path, output_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to numpy arrays\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = os.path.join(output_dir_train, img_name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    X_train.append(image)\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = os.path.join(output_dir_test, img_name + \".jpg\")\n",
    "    image = cv2.imread(image_path)\n",
    "    X_test.append(image)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Extract labels directly without one-hot encoding\n",
    "y_train = train_data[\"Retinopathy grade\"].values\n",
    "y_test = test_data[\"Retinopathy grade\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize\n",
    "\n",
    "Colour Normalisation: The retina images were captured from patients of different age, and various ethnicity [58], at different levels of lighting in the fundus image. These conditions have an effect on the value of pixel intensity of each image and create unnecessary variation in the image [58]. To overcome this, the retina images were normalised by normalising each channel of RGB images. For the normalization, we subtract the mean, and after that, divide the variance of the images [25], as shown in Equation (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation\n",
    "\n",
    "Online data augmentation was adopted to enlarge the training dataset and to improve the generalisation and performance of the CNN. The images were augmented by performing rotation, flipping, shearing, and translation, as well as randomly darkening and brightening them, as shown in Figure 8. The augmentation parameters are presented in Table 5. \n",
    "\n",
    "Data augmentation parameters.\n",
    "\n",
    "Transformation Type\tDescription\n",
    "\n",
    "Rotation\tRotate the image randomly between (−35∘, 35∘).\n",
    "\n",
    "Flipping\tHorizontal and vertical flip for the images.\n",
    "\n",
    "Shearing\tRandomly Shear images with angle between −15∘ and 15∘.\n",
    "\n",
    "Translation\tRandomly with shift between −10% and 10% of pixels.\n",
    "\n",
    "Brightness range\tRandomly darken the image and brighten. The values less than 1.0 the image darken whereas values larger than 1.0 brighten the image. The used values (0.25, 1.25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# augmentation function\n",
    "def augment(image, label):\n",
    "    # Rotation (-35 degrees to 35 degrees)\n",
    "    degrees = tf.random.uniform([], -35, 35, dtype=tf.float32)\n",
    "    image = tfa.image.rotate(image, math.radians(degrees))\n",
    "\n",
    "    # Flipping (randomly apply horizontal and vertical flipping)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "\n",
    "    # Shearing (TensorFlow Addons needed)\n",
    "    shearing = tf.random.uniform(\n",
    "        [], -0.15, 0.15, dtype=tf.float32\n",
    "    )  # Shear angle in radians\n",
    "    image = tfa.image.shear_x(image, shearing, 0)  # Apply shearing in X direction\n",
    "    image = tfa.image.shear_y(image, shearing, 0)  # Apply shearing in Y direction\n",
    "\n",
    "    # Translation (randomly shift image within ±10% of its height and width)\n",
    "    width_shift = tf.random.uniform([], -0.1, 0.1) * tf.shape(image)[1]\n",
    "    height_shift = tf.random.uniform([], -0.1, 0.1) * tf.shape(image)[0]\n",
    "    image = tfa.image.translate(image, [width_shift, height_shift])\n",
    "\n",
    "    # Brightness (adjust brightness by a factor chosen from [0.25, 1.25])\n",
    "    image = tf.image.random_brightness(\n",
    "        image, max_delta=0.5\n",
    "    )  # Note: delta adjusted to fit the [0.25, 1.25] range more closely\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "mean = np.mean(X_train, axis=(0, 1, 2))\n",
    "std = np.std(X_train, axis=(0, 1, 2))\n",
    "\n",
    "\n",
    "def normalize_image_with_label(image, label, mean, std):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    normalized_image = (image - mean) / (std + 1e-7)\n",
    "    return normalized_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation and normalization in data pipeline\n",
    "\n",
    "# Preparing the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda image, label: (augment(image), label), num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "# Applying the normalization function to a dataset\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda image, label: normalize_image_with_label(image, label, mean, std),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preparing the testing dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda image, label: normalize_image_with_label(image, label, mean, std),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 512, 3)\n",
    "num_classes = 5\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CNN 512 no dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *CNN512* \n",
    "\n",
    "Layer\tOperator\tLayer Details\n",
    "\n",
    "Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "\n",
    "Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "\n",
    "Layer 2\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 3\tRelu layer\t-\n",
    "\n",
    "Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "\n",
    "Layer 6\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 7\tRelu layer\t-\n",
    "\n",
    "Layer 8\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 10\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 11\tRelu layer\t-\n",
    "\n",
    "Layer 12\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "\n",
    "Layer 14\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 15\tRelu layer\t-\n",
    "\n",
    "Layer 16\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "\n",
    "Layer 18\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 19\tRelu layer\t-\n",
    "\n",
    "Layer 20\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "\n",
    "Layer 22\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 23\tRelu layer\t-\n",
    "\n",
    "Layer 24\tMax Pooling layer\tPooling size (2,2)\n",
    "\n",
    "Layer 25\tFlatten layer\t-\n",
    "\n",
    "Layer 26\tFC layer\tNeurons number = 1000\n",
    "\n",
    "Layer 27\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 28\tRelu layer\t-\n",
    "\n",
    "Layer 29\tFC layer\tNeurons number = 500\n",
    "\n",
    "Layer 30\tBatch Normalization layer\t-\n",
    "\n",
    "Layer 31\tRelu layer\t-\n",
    "\n",
    "Layer 32\tFC layer\tWith SoftMax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn512_nodropout = models.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        # Input Layer\tZero Padding layer\tPadding (2,2)\n",
    "        layers.ZeroPadding2D(padding=(2, 2)),\n",
    "        # Layer 1\t2D CONV layer\tKernel number = 32, kernel size = 3\n",
    "        layers.Conv2D(32, kernel_size=(3, 3)),\n",
    "        # Layer 2\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 3\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 4\tMax Pooling layer\tPooling size (2,2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 5\t2D CONV layer\tKernel number = 64, kernel size = 3\n",
    "        layers.Conv2D(64, kernel_size=(3, 3)),\n",
    "        # Layer 6\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 7\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 8\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 9\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3)),\n",
    "        # Layer 10\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 11\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 12\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 13\t2D CONV layer\tKernel number = 96, kernel size = 3\n",
    "        layers.Conv2D(96, kernel_size=(3, 3)),\n",
    "        # Layer 14\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 15\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 16\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 17\t2D CONV layer\tKernel number = 128, kernel size = 3\n",
    "        layers.Conv2D(128, kernel_size=(3, 3)),\n",
    "        # Layer 18\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 19\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 20\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 21\t2D CONV layer\tKernel number = 200, kernel size = 3\n",
    "        layers.Conv2D(200, kernel_size=(3, 3)),\n",
    "        # Layer 22\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 23\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # Layer 24\tMax Pooling layer\tPooling size (2, 2)\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Layer 25\tFlatten layer\t-\n",
    "        layers.Flatten(),\n",
    "        # Layer 26\tFC layer\tNeurons number = 1000\n",
    "        layers.Dense(1000),\n",
    "        # Layer 27\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 28\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # layers.Dropout(0.5),\n",
    "        # Layer 29\tFC layer\tNeurons number = 500\n",
    "        layers.Dense(500),\n",
    "        # Layer 30\tBatch Normalization layer\t-\n",
    "        layers.BatchNormalization(),\n",
    "        # Layer 31\tRelu layer\t-\n",
    "        layers.Activation(\"relu\"),\n",
    "        # layers.Dropout(0.5),\n",
    "        # Layer 32\tFC layer with Softmax activatio\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *The hyperparameter configuration of CNNs.*\n",
    "\n",
    "Configuration\tValues\n",
    "\n",
    "Optimizer\tSGD\n",
    "\n",
    "Momentum\t0.9\n",
    "\n",
    "Max Learning rate\t1×10−1 in custom CNNs\n",
    "\n",
    "1×10−2 in EfficientNetB0 (exclude since not using EfficientNetB0)\n",
    "\n",
    "Base Learning rate\t1×10−4\n",
    "\n",
    "Mode\ttriangular\n",
    "\n",
    "Class weight\tauto\n",
    "\n",
    "Dropout\t0.5 (exclude dropout)\n",
    "\n",
    "Augmentation\t20 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "cnn512_nodropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# create class weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# create triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "history_cnn512_nodropout = cnn512_nodropout.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=math.ceil(len(X_train) / 32),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn512_nodropout.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
