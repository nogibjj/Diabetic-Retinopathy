{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN512 Model (without dropout) - luminosity + normalized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons as tfa\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"train_split_key_transformation.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "test_data = data[data[\"split\"] == \"test\"]\n",
    "\n",
    "# Define output directories for preprocessed and normalized images\n",
    "preprocessed_output_dir_train = \"preprocessed_images_train/\"\n",
    "preprocessed_output_dir_test = \"preprocessed_images_test/\"\n",
    "normalized_output_dir_train = \"normalized_images_train/\"\n",
    "normalized_output_dir_test = \"normalized_images_test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(preprocessed_output_dir_train, exist_ok=True)\n",
    "os.makedirs(preprocessed_output_dir_test, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_train, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_test, exist_ok=True)\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, output_dir):\n",
    "    # Load and resize the image to 512x512\n",
    "    resized_image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(resized_image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    l_eq = clahe.apply(l)\n",
    "\n",
    "    # Merge back LAB channels\n",
    "    lab_eq = cv2.merge((l_eq, a, b))\n",
    "    enhanced_image = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Image noise removal using Gaussian filter\n",
    "    filtered_image = gaussian_filter(enhanced_image, sigma=1)\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# Define augmentation function\n",
    "def augment_image(image):\n",
    "    # Apply augmentation operations\n",
    "    image = tfa.image.rotate(image, tf.random.uniform([], -35, 35, dtype=tf.float32))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tfa.image.shear_x(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.shear_y(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.translate(\n",
    "        image, [tf.random.uniform([], -0.1, 0.1), tf.random.uniform([], -0.1, 0.1)]\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_images(images, factor=0.5):\n",
    "    normalized_images = []\n",
    "    for image in images:\n",
    "        # Ensure the image is in float32 format\n",
    "        image_float32 = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Compute mean intensity for all channels\n",
    "        mean_intensity = np.mean(image_float32)\n",
    "\n",
    "        # Compute scaling factor for intensity adjustment\n",
    "        scaling_factor = factor / mean_intensity\n",
    "\n",
    "        # Scale each channel independently\n",
    "        balanced_image = np.clip(image_float32 * scaling_factor, 0, 1)\n",
    "\n",
    "        # Convert image back to uint8 format\n",
    "        normalized_image = (balanced_image * 255).astype(np.uint8)\n",
    "\n",
    "        normalized_images.append(normalized_image)\n",
    "\n",
    "    return np.array(normalized_images)\n",
    "\n",
    "\n",
    "# Apply preprocessing to training images\n",
    "X_train_paths = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"train/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_train)\n",
    "    X_train_paths.append(output_path)\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths = []\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"test/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_test)\n",
    "    X_test_paths.append(output_path)\n",
    "\n",
    "# Load preprocessed images\n",
    "X_train = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "# Normalize and save training images\n",
    "for i, image_path in enumerate(X_train_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_train, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "# Normalize and save testing images\n",
    "for i, image_path in enumerate(X_test_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_test, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "# Apply augmentation to training images\n",
    "X_train_augmented = []\n",
    "\n",
    "for image in X_train:\n",
    "    augmented_image = augment_image(image)\n",
    "    X_train_augmented.append(augmented_image)\n",
    "\n",
    "# Convert augmented images to numpy array\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_data[\"Retinopathy grade new\"].values\n",
    "y_test = test_data[\"Retinopathy grade new\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    ZeroPadding2D,\n",
    "    Activation,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# Define the CNN512 model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer (Zero Padding)\n",
    "model.add(ZeroPadding2D(padding=(2, 2), input_shape=(512, 512, 3)))\n",
    "\n",
    "# Layer 1, 2, 3\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 4\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 5, 6, 7\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 8\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 9, 10, 11\n",
    "model.add(Conv2D(96, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 12\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 13, 14, 15\n",
    "model.add(Conv2D(96, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 16\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 17, 18, 19\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 20\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 21, 22, 23\n",
    "model.add(Conv2D(200, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 24\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 25\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 26, 27, 28\n",
    "model.add(Dense(1000))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 29, 30, 31\n",
    "model.add(Dense(500))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 32\n",
    "model.add(Dense(4, activation=\"softmax\"))  # Assuming 4 classes for Retinopathy grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\afraa\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "16/16 [==============================] - 57s 3s/step - loss: 1.8583 - accuracy: 0.1565 - val_loss: 1.4029 - val_accuracy: 0.2917 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 2.0983 - accuracy: 0.2398 - val_loss: 1507.9619 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 53s 3s/step - loss: 2.6812 - accuracy: 0.2622 - val_loss: 5949.9829 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 4.0970 - accuracy: 0.2195 - val_loss: 73.9379 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 3.6032 - accuracy: 0.2785 - val_loss: 28.4052 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 60s 4s/step - loss: 3.2696 - accuracy: 0.2297 - val_loss: 14.9882 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 60s 4s/step - loss: 3.2252 - accuracy: 0.2439 - val_loss: 38.8932 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 2.0564 - accuracy: 0.2520 - val_loss: 5.5130 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 65s 4s/step - loss: 1.7342 - accuracy: 0.2398 - val_loss: 19.3257 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 71s 4s/step - loss: 1.7320 - accuracy: 0.2256 - val_loss: 2.4324 - val_accuracy: 0.2083 - lr: 0.0900\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 70s 4s/step - loss: 1.6263 - accuracy: 0.2053 - val_loss: 1.3823 - val_accuracy: 0.3333 - lr: 0.1000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 71s 4s/step - loss: 1.4068 - accuracy: 0.1646 - val_loss: 1.3918 - val_accuracy: 0.2917 - lr: 0.0900\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 70s 4s/step - loss: 1.4794 - accuracy: 0.1789 - val_loss: 1.4085 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 70s 4s/step - loss: 1.4038 - accuracy: 0.1402 - val_loss: 1.3869 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 69s 4s/step - loss: 1.4349 - accuracy: 0.1443 - val_loss: 1.4056 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 63s 4s/step - loss: 1.3449 - accuracy: 0.3354 - val_loss: 1.3827 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 1.3308 - accuracy: 0.3557 - val_loss: 1.3816 - val_accuracy: 0.2917 - lr: 0.0401\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 1.2987 - accuracy: 0.2520 - val_loss: 1.3779 - val_accuracy: 0.2917 - lr: 0.0301\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 1.2995 - accuracy: 0.2561 - val_loss: 1.3734 - val_accuracy: 0.3333 - lr: 0.0201\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 1.2825 - accuracy: 0.3557 - val_loss: 1.3767 - val_accuracy: 0.2917 - lr: 0.0101\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 61s 4s/step - loss: 1.2820 - accuracy: 0.3699 - val_loss: 1.3790 - val_accuracy: 0.2917 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 64s 4s/step - loss: 1.2587 - accuracy: 0.3801 - val_loss: 1.3814 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 59s 4s/step - loss: 1.2599 - accuracy: 0.3252 - val_loss: 1.3795 - val_accuracy: 0.3333 - lr: 0.0201\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 58s 4s/step - loss: 1.2774 - accuracy: 0.2988 - val_loss: 1.3799 - val_accuracy: 0.3750 - lr: 0.0301\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 55s 3s/step - loss: 1.2482 - accuracy: 0.3415 - val_loss: 1.3788 - val_accuracy: 0.2917 - lr: 0.0401\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.2749 - accuracy: 0.2398 - val_loss: 1.3701 - val_accuracy: 0.2083 - lr: 0.0501\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3651 - accuracy: 0.1870 - val_loss: 1.4381 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3810 - accuracy: 0.2480 - val_loss: 1.4570 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.2852 - accuracy: 0.2439 - val_loss: 1.3923 - val_accuracy: 0.3333 - lr: 0.0800\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3639 - accuracy: 0.3028 - val_loss: 1.4041 - val_accuracy: 0.3333 - lr: 0.0900\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3656 - accuracy: 0.2967 - val_loss: 1.4851 - val_accuracy: 0.2500 - lr: 0.1000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3023 - accuracy: 0.2581 - val_loss: 1.3925 - val_accuracy: 0.2083 - lr: 0.0900\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.2566 - accuracy: 0.2541 - val_loss: 1.4738 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.2131 - accuracy: 0.3211 - val_loss: 1.4706 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.3618 - accuracy: 0.2663 - val_loss: 1.3802 - val_accuracy: 0.1667 - lr: 0.0600\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.1660 - accuracy: 0.2073 - val_loss: 1.4530 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 1.0720 - accuracy: 0.3740 - val_loss: 1.5440 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 55s 3s/step - loss: 1.1033 - accuracy: 0.3923 - val_loss: 1.6305 - val_accuracy: 0.2917 - lr: 0.0301\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 54s 3s/step - loss: 0.9855 - accuracy: 0.3740 - val_loss: 1.8224 - val_accuracy: 0.2917 - lr: 0.0201\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 0.9304 - accuracy: 0.4309 - val_loss: 1.7685 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 0.9220 - accuracy: 0.4451 - val_loss: 1.8278 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 0.8931 - accuracy: 0.4695 - val_loss: 1.8792 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 0.8496 - accuracy: 0.4695 - val_loss: 1.8533 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 0.8373 - accuracy: 0.4715 - val_loss: 1.9899 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 0.8095 - accuracy: 0.5264 - val_loss: 2.0141 - val_accuracy: 0.2917 - lr: 0.0401\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 0.9268 - accuracy: 0.4898 - val_loss: 2.1917 - val_accuracy: 0.2917 - lr: 0.0501\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.0401 - accuracy: 0.4492 - val_loss: 1.8286 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 0.9203 - accuracy: 0.4776 - val_loss: 2.4791 - val_accuracy: 0.2083 - lr: 0.0700\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 1.0023 - accuracy: 0.4858 - val_loss: 1.8254 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 1.1234 - accuracy: 0.4248 - val_loss: 2.5709 - val_accuracy: 0.3750 - lr: 0.0900\n",
      "1/1 [==============================] - 1s 694ms/step - loss: 2.5709 - accuracy: 0.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5709457397460938, 0.375]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# Maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "# Create class weights\n",
    "# Convert y_train to a hashable data type\n",
    "y_train_list = list(y_train)\n",
    "classes = np.unique(y_train_list)\n",
    "\n",
    "# Create class weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_list)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# Define triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train_augmented,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 725ms/step - loss: 2.5709 - accuracy: 0.3750\n",
      "IDRiD Test Accuracy (without dropout): 0.375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_norm, test_accuracy_norm = model.evaluate(X_test, y_test)\n",
    "print(\"IDRiD Test Accuracy (without dropout):\", test_accuracy_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN512 Model (without dropout) - luminosity + normalized images for MESSIDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image_path, output_dir):\n",
    "    # Load and resize the image to 512x512\n",
    "    resized_image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(resized_image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    l_eq = clahe.apply(l)\n",
    "\n",
    "    # Merge back LAB channels\n",
    "    lab_eq = cv2.merge((l_eq, a, b))\n",
    "    enhanced_image = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Image noise removal using Gaussian filter\n",
    "    filtered_image = gaussian_filter(enhanced_image, sigma=1)\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_m = pd.read_excel(\"Messidor_Data/messidor_mapping.xlsx\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "test_data_messidor = data_m[data_m[\"Split\"] == \"Test\"]\n",
    "\n",
    "# Define output directories for preprocessed and normalized images\n",
    "preprocessed_output_dir_test_m = \"Messidor_Data/messidor_preprocessed_images_test/\"\n",
    "normalized_output_dir_test_m = \"Messidor_Data/messidor_normalized_images_test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(preprocessed_output_dir_test_m, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_test_m, exist_ok=True)\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths_m = []\n",
    "\n",
    "for img_name in test_data_messidor[\"Image_ID\"]:\n",
    "    image_name_without_extension = os.path.splitext(img_name)[0]  # Remove extension\n",
    "    image_path = f\"Messidor_Data/messidor_test/{image_name_without_extension}.jpg\"  # Append .jpg extension\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_test_m)\n",
    "    X_test_paths_m.append(output_path)\n",
    "\n",
    "# Load preprocessed images\n",
    "X_test_m = np.array([cv2.imread(img_path) for img_path in X_test_paths_m])\n",
    "\n",
    "# Normalize and save testing images\n",
    "for i, image_path in enumerate(X_test_paths_m):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_test_m, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "# Extract labels\n",
    "y_test_m = test_data_messidor[\"Retinopathy_Grade\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 539ms/step - loss: 3.3169 - accuracy: 0.2500\n",
      "Messidor Test Accuracy (without dropout): 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_norm_m, test_accuracy_norm_m = model.evaluate(X_test_m, y_test_m)\n",
    "print(\"Messidor Test Accuracy (without dropout):\", test_accuracy_norm_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN512 Model (with dropout) - luminosity + normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN512 model architecture\n",
    "model_dropout = Sequential()\n",
    "\n",
    "# Input Layer (Zero Padding)\n",
    "model_dropout.add(ZeroPadding2D(padding=(2, 2), input_shape=(512, 512, 3)))\n",
    "\n",
    "# Layer 1, 2, 3\n",
    "model_dropout.add(Conv2D(32, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 4\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 5, 6, 7\n",
    "model_dropout.add(Conv2D(64, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 8\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 9, 10, 11\n",
    "model_dropout.add(Conv2D(96, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 12\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 13, 14, 15\n",
    "model_dropout.add(Conv2D(96, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 16\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 17, 18, 19\n",
    "model_dropout.add(Conv2D(128, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 20\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 21, 22, 23\n",
    "model_dropout.add(Conv2D(200, (3, 3)))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 24\n",
    "model_dropout.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 25\n",
    "model_dropout.add(Flatten())\n",
    "\n",
    "# Layer 26, 27, 28\n",
    "model_dropout.add(Dense(1000))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "model_dropout.add(Dropout(0.5))\n",
    "\n",
    "# Layer 29, 30, 31\n",
    "model_dropout.add(Dense(500))\n",
    "model_dropout.add(BatchNormalization())\n",
    "model_dropout.add(Activation(\"relu\"))\n",
    "model_dropout.add(Dropout(0.5))\n",
    "\n",
    "# Layer 32\n",
    "model_dropout.add(\n",
    "    Dense(4, activation=\"softmax\")\n",
    ")  # Assuming 4 classes for Retinopathy grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 53s 3s/step - loss: 1.9897 - accuracy: 0.2561 - val_loss: 1.7209 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 2.1751 - accuracy: 0.2154 - val_loss: 6487.3398 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 3.1964 - accuracy: 0.2073 - val_loss: 2846.6260 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 3.7356 - accuracy: 0.2337 - val_loss: 176.2586 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 4.8762 - accuracy: 0.2215 - val_loss: 11073.7002 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 4.9660 - accuracy: 0.2520 - val_loss: 1650.1035 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 8.4475 - accuracy: 0.2175 - val_loss: 245.0129 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 6.2174 - accuracy: 0.2785 - val_loss: 78.1243 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 7.7829 - accuracy: 0.2256 - val_loss: 152.0026 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 4.4096 - accuracy: 0.2642 - val_loss: 551.6795 - val_accuracy: 0.2500 - lr: 0.0900\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 3.8616 - accuracy: 0.2947 - val_loss: 43.1833 - val_accuracy: 0.2500 - lr: 0.1000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 2.3973 - accuracy: 0.2642 - val_loss: 32.7934 - val_accuracy: 0.3750 - lr: 0.0900\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 2.0771 - accuracy: 0.2459 - val_loss: 7.1264 - val_accuracy: 0.2083 - lr: 0.0800\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.4872 - accuracy: 0.1606 - val_loss: 4.2427 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 1.3936 - accuracy: 0.2642 - val_loss: 2.0999 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.3788 - accuracy: 0.2866 - val_loss: 1.4857 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.4277 - accuracy: 0.2195 - val_loss: 1.3776 - val_accuracy: 0.2917 - lr: 0.0401\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.4137 - accuracy: 0.1951 - val_loss: 1.3841 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 1.3754 - accuracy: 0.1667 - val_loss: 1.3892 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.3800 - accuracy: 0.1951 - val_loss: 1.3931 - val_accuracy: 0.2917 - lr: 0.0101\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.3748 - accuracy: 0.2033 - val_loss: 1.3982 - val_accuracy: 0.2917 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 1.3782 - accuracy: 0.1931 - val_loss: 1.4003 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.4039 - accuracy: 0.2236 - val_loss: 1.4027 - val_accuracy: 0.1667 - lr: 0.0201\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.3625 - accuracy: 0.2520 - val_loss: 1.4137 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 1.3906 - accuracy: 0.2073 - val_loss: 1.3926 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3515 - accuracy: 0.1382 - val_loss: 1.3940 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3801 - accuracy: 0.1565 - val_loss: 1.3968 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3603 - accuracy: 0.1748 - val_loss: 1.3989 - val_accuracy: 0.2917 - lr: 0.0700\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3965 - accuracy: 0.1870 - val_loss: 1.3962 - val_accuracy: 0.4167 - lr: 0.0800\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.4089 - accuracy: 0.1972 - val_loss: 1.3857 - val_accuracy: 0.2500 - lr: 0.0900\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3892 - accuracy: 0.1606 - val_loss: 1.3945 - val_accuracy: 0.1250 - lr: 0.1000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.4012 - accuracy: 0.3699 - val_loss: 1.3931 - val_accuracy: 0.2083 - lr: 0.0900\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3752 - accuracy: 0.2358 - val_loss: 1.3981 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3528 - accuracy: 0.1748 - val_loss: 1.3983 - val_accuracy: 0.1250 - lr: 0.0700\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3753 - accuracy: 0.2378 - val_loss: 1.3920 - val_accuracy: 0.1667 - lr: 0.0600\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.4094 - accuracy: 0.2256 - val_loss: 1.3800 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.4368 - accuracy: 0.1016 - val_loss: 1.4053 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3747 - accuracy: 0.1667 - val_loss: 1.3867 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3638 - accuracy: 0.1728 - val_loss: 1.3888 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3859 - accuracy: 0.1890 - val_loss: 1.3880 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3550 - accuracy: 0.1870 - val_loss: 1.3879 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3718 - accuracy: 0.1809 - val_loss: 1.3881 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 50s 3s/step - loss: 1.3618 - accuracy: 0.1646 - val_loss: 1.3912 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3778 - accuracy: 0.1159 - val_loss: 1.3902 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 1.3639 - accuracy: 0.1423 - val_loss: 1.3958 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.3445 - accuracy: 0.1667 - val_loss: 1.3966 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3734 - accuracy: 0.2093 - val_loss: 1.3904 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3573 - accuracy: 0.2114 - val_loss: 1.3958 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3655 - accuracy: 0.1484 - val_loss: 1.4033 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.3869 - accuracy: 0.2053 - val_loss: 1.3847 - val_accuracy: 0.2500 - lr: 0.0900\n",
      "1/1 [==============================] - 1s 678ms/step - loss: 1.3847 - accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3847036361694336, 0.25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# Maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "# Create class weights\n",
    "# Convert y_train to a hashable data type\n",
    "y_train_list = list(y_train)\n",
    "classes = np.unique(y_train_list)\n",
    "\n",
    "# Create class weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_list)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# Define triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model_dropout.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model_dropout.fit(\n",
    "    X_train_augmented,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model_dropout.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 714ms/step - loss: 1.3847 - accuracy: 0.2500\n",
      "IDRiD Test Accuracy (with dropout): 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_dropout.evaluate(X_test, y_test)\n",
    "print(\"IDRiD Test Accuracy (with dropout):\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN512 Model (with dropout) - luminosity + normalized images MESSIDOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 678ms/step - loss: 1.7672 - accuracy: 0.2500\n",
      "Messidor Test Accuracy (with dropout): 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_dropout_m, test_accuracy_dropout_m = model_dropout.evaluate(\n",
    "    X_test_m, y_test_m\n",
    ")\n",
    "print(\"Messidor Test Accuracy (with dropout):\", test_accuracy_dropout_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Train model on base images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"train_split_key_transformation.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "test_data = data[data[\"split\"] == \"test\"]\n",
    "\n",
    "# Define output directories for preprocessed and normalized images\n",
    "preprocessed_output_dir_train = \"preprocessed_images_train/\"\n",
    "preprocessed_output_dir_test = \"preprocessed_images_test/\"\n",
    "normalized_output_dir_train = \"normalized_images_train/\"\n",
    "normalized_output_dir_test = \"normalized_images_test/\"\n",
    "\n",
    "\n",
    "# Define augmentation function\n",
    "def augment_image(image):\n",
    "    # Apply augmentation operations\n",
    "    image = tfa.image.rotate(image, tf.random.uniform([], -35, 35, dtype=tf.float32))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tfa.image.shear_x(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.shear_y(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.translate(\n",
    "        image, [tf.random.uniform([], -0.1, 0.1), tf.random.uniform([], -0.1, 0.1)]\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Apply preprocessing to training images\n",
    "X_train_paths = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"train/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_train)\n",
    "    X_train_paths.append(output_path)\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths = []\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"test/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_test)\n",
    "    X_test_paths.append(output_path)\n",
    "\n",
    "# Load preprocessed images\n",
    "X_train = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "# Normalize and save training images\n",
    "for i, image_path in enumerate(X_train_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_train, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "# Normalize and save testing images\n",
    "for i, image_path in enumerate(X_test_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_test, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply preprocessing to training images\n",
    "X_train_paths = [\"train/\" + img_name + \".jpg\" for img_name in train_data[\"Image name\"]]\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths = [\"test/\" + img_name + \".jpg\" for img_name in test_data[\"Image name\"]]\n",
    "\n",
    "# Load images\n",
    "X_train = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_data[\"Retinopathy grade new\"].values\n",
    "y_test = test_data[\"Retinopathy grade new\"].values\n",
    "\n",
    "# Apply augmentation to training images\n",
    "X_train_augmented = []\n",
    "\n",
    "for image in X_train:\n",
    "    augmented_image = augment_image(image)\n",
    "    X_train_augmented.append(augmented_image)\n",
    "\n",
    "# Convert augmented images to numpy array\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_data[\"Retinopathy grade new\"].values\n",
    "y_test = test_data[\"Retinopathy grade new\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"train_split_key_transformation.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "test_data = data[data[\"split\"] == \"test\"]\n",
    "\n",
    "\n",
    "# Define augmentation function\n",
    "def augment_image(image):\n",
    "    # Apply augmentation operations\n",
    "    image = tfa.image.rotate(image, tf.random.uniform([], -35, 35, dtype=tf.float32))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tfa.image.shear_x(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.shear_y(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.translate(\n",
    "        image, [tf.random.uniform([], -0.1, 0.1), tf.random.uniform([], -0.1, 0.1)]\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Apply preprocessing to training images NOT\n",
    "X_train_paths = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"train/\" + img_name + \".jpg\"\n",
    "    X_train_paths.append(image_path)\n",
    "\n",
    "# Apply preprocessing to testing images NOT\n",
    "X_test_paths = []\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"test/\" + img_name + \".jpg\"\n",
    "    X_test_paths.append(image_path)\n",
    "\n",
    "X_train = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "# Apply augmentation to training images\n",
    "X_train_augmented = []\n",
    "\n",
    "for image in X_train:\n",
    "    augmented_image = augment_image(image)\n",
    "    X_train_augmented.append(augmented_image)\n",
    "\n",
    "# Convert augmented images to numpy array\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_data[\"Retinopathy grade new\"].values\n",
    "y_test = test_data[\"Retinopathy grade new\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN512 model architecture\n",
    "model_base = Sequential()\n",
    "\n",
    "# Input Layer (Zero Padding)\n",
    "model_base.add(ZeroPadding2D(padding=(2, 2), input_shape=(512, 512, 3)))\n",
    "\n",
    "# Layer 1, 2, 3\n",
    "model_base.add(Conv2D(32, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 4\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 5, 6, 7\n",
    "model_base.add(Conv2D(64, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 8\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 9, 10, 11\n",
    "model_base.add(Conv2D(96, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 12\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 13, 14, 15\n",
    "model_base.add(Conv2D(96, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 16\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 17, 18, 19\n",
    "model_base.add(Conv2D(128, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 20\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 21, 22, 23\n",
    "model_base.add(Conv2D(200, (3, 3)))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 24\n",
    "model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 25\n",
    "model_base.add(Flatten())\n",
    "\n",
    "# Layer 26, 27, 28\n",
    "model_base.add(Dense(1000))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 29, 30, 31\n",
    "model_base.add(Dense(500))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Activation(\"relu\"))\n",
    "\n",
    "# Layer 32\n",
    "model_base.add(\n",
    "    Dense(4, activation=\"softmax\")\n",
    ")  # Assuming 4 classes for Retinopathy grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 1.7101 - accuracy: 0.2724 - val_loss: 1.4572 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 42s 3s/step - loss: 1.8931 - accuracy: 0.2520 - val_loss: 1451.8940 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 43s 3s/step - loss: 2.2034 - accuracy: 0.2337 - val_loss: 2569.3379 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 2.1746 - accuracy: 0.2663 - val_loss: 623.7988 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 3.8713 - accuracy: 0.2703 - val_loss: 756.2630 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 49s 3s/step - loss: 2.6429 - accuracy: 0.2744 - val_loss: 1139.7524 - val_accuracy: 0.2500 - lr: 0.0501\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 51s 3s/step - loss: 3.9907 - accuracy: 0.2276 - val_loss: 477.6220 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 61s 4s/step - loss: 2.3055 - accuracy: 0.2215 - val_loss: 142.8611 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 61s 4s/step - loss: 2.0727 - accuracy: 0.2419 - val_loss: 209.1921 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 65s 4s/step - loss: 1.7738 - accuracy: 0.3293 - val_loss: 14.1579 - val_accuracy: 0.2500 - lr: 0.0900\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 58s 4s/step - loss: 1.5658 - accuracy: 0.2114 - val_loss: 3.3412 - val_accuracy: 0.2500 - lr: 0.1000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 52s 3s/step - loss: 1.6535 - accuracy: 0.1809 - val_loss: 3.5911 - val_accuracy: 0.2500 - lr: 0.0900\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 65s 4s/step - loss: 1.4008 - accuracy: 0.3394 - val_loss: 1.6608 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 60s 4s/step - loss: 1.3926 - accuracy: 0.2764 - val_loss: 1.4077 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 61s 4s/step - loss: 1.4461 - accuracy: 0.1768 - val_loss: 1.8435 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 63s 4s/step - loss: 1.3247 - accuracy: 0.1362 - val_loss: 1.3839 - val_accuracy: 0.2917 - lr: 0.0501\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 64s 4s/step - loss: 1.4071 - accuracy: 0.4329 - val_loss: 1.3892 - val_accuracy: 0.2500 - lr: 0.0401\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 63s 4s/step - loss: 1.3774 - accuracy: 0.1687 - val_loss: 1.4194 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 56s 4s/step - loss: 1.3441 - accuracy: 0.2073 - val_loss: 1.4109 - val_accuracy: 0.2500 - lr: 0.0201\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 52s 3s/step - loss: 1.2974 - accuracy: 0.3110 - val_loss: 1.4043 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 48s 3s/step - loss: 1.2796 - accuracy: 0.3171 - val_loss: 1.3904 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.3001 - accuracy: 0.2663 - val_loss: 1.3851 - val_accuracy: 0.2500 - lr: 0.0101\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.2930 - accuracy: 0.3374 - val_loss: 1.3693 - val_accuracy: 0.2917 - lr: 0.0201\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 47s 3s/step - loss: 1.2798 - accuracy: 0.2785 - val_loss: 1.3701 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.2720 - accuracy: 0.2967 - val_loss: 1.3684 - val_accuracy: 0.3333 - lr: 0.0401\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.2764 - accuracy: 0.4106 - val_loss: 1.3645 - val_accuracy: 0.3750 - lr: 0.0501\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.2372 - accuracy: 0.3313 - val_loss: 1.4512 - val_accuracy: 0.2500 - lr: 0.0600\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.2324 - accuracy: 0.3089 - val_loss: 1.3978 - val_accuracy: 0.3333 - lr: 0.0700\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.3002 - accuracy: 0.2805 - val_loss: 1.5758 - val_accuracy: 0.3333 - lr: 0.0800\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.3237 - accuracy: 0.3374 - val_loss: 1.4951 - val_accuracy: 0.3333 - lr: 0.0900\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.2626 - accuracy: 0.3008 - val_loss: 1.3667 - val_accuracy: 0.2917 - lr: 0.1000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.2655 - accuracy: 0.3171 - val_loss: 1.4210 - val_accuracy: 0.3333 - lr: 0.0900\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.1770 - accuracy: 0.3313 - val_loss: 1.7440 - val_accuracy: 0.3750 - lr: 0.0800\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.1444 - accuracy: 0.4329 - val_loss: 1.9389 - val_accuracy: 0.2500 - lr: 0.0700\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.1107 - accuracy: 0.4553 - val_loss: 1.4211 - val_accuracy: 0.3750 - lr: 0.0600\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0826 - accuracy: 0.3354 - val_loss: 1.6226 - val_accuracy: 0.3333 - lr: 0.0501\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.1102 - accuracy: 0.4634 - val_loss: 1.6008 - val_accuracy: 0.2083 - lr: 0.0401\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0196 - accuracy: 0.3963 - val_loss: 1.5203 - val_accuracy: 0.1667 - lr: 0.0301\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0174 - accuracy: 0.4187 - val_loss: 1.6176 - val_accuracy: 0.2917 - lr: 0.0201\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.9858 - accuracy: 0.4451 - val_loss: 1.4372 - val_accuracy: 0.4167 - lr: 0.0101\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.9689 - accuracy: 0.4675 - val_loss: 1.4758 - val_accuracy: 0.4167 - lr: 1.0000e-04\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0328 - accuracy: 0.4553 - val_loss: 1.4668 - val_accuracy: 0.3750 - lr: 0.0101\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 0.9568 - accuracy: 0.4675 - val_loss: 1.5756 - val_accuracy: 0.3333 - lr: 0.0201\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.9868 - accuracy: 0.4512 - val_loss: 1.7513 - val_accuracy: 0.2500 - lr: 0.0301\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 46s 3s/step - loss: 1.0251 - accuracy: 0.4024 - val_loss: 1.6368 - val_accuracy: 0.3333 - lr: 0.0401\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.9713 - accuracy: 0.4533 - val_loss: 1.6662 - val_accuracy: 0.3333 - lr: 0.0501\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 0.9832 - accuracy: 0.4350 - val_loss: 1.7755 - val_accuracy: 0.2917 - lr: 0.0600\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0772 - accuracy: 0.4512 - val_loss: 2.9679 - val_accuracy: 0.2917 - lr: 0.0700\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.0910 - accuracy: 0.4106 - val_loss: 2.0653 - val_accuracy: 0.2500 - lr: 0.0800\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.1421 - accuracy: 0.4004 - val_loss: 2.1721 - val_accuracy: 0.2917 - lr: 0.0900\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 2.1721 - accuracy: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.172149896621704, 0.2916666567325592]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# Maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "# Create class weights\n",
    "# Convert y_train to a hashable data type\n",
    "y_train_list = list(y_train)\n",
    "classes = np.unique(y_train_list)\n",
    "\n",
    "# Create class weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_list)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# Define triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model_base.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model_base.fit(\n",
    "    X_train_augmented,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model_base.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 778ms/step - loss: 2.1721 - accuracy: 0.2917\n",
      "IDRiD Test Accuracy BASE images (without dropout): 0.2916666567325592\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_base, test_accuracy_base = model_base.evaluate(X_test, y_test)\n",
    "print(\"IDRiD Test Accuracy BASE images (without dropout):\", test_accuracy_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_m = pd.read_excel(\"Messidor_Data/messidor_mapping.xlsx\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "test_data_messidor = data_m[data_m[\"Split\"] == \"Test\"]\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths_m = []\n",
    "\n",
    "for img_name in test_data_messidor[\"Image_ID\"]:\n",
    "    image_name_without_extension = os.path.splitext(img_name)[0]  # Remove extension\n",
    "    image_path = f\"Messidor_Data/messidor_test/{image_name_without_extension}.jpg\"  # Append .jpg extension\n",
    "    X_test_paths_m.append(image_path)\n",
    "\n",
    "# Load preprocessed images\n",
    "X_test_m = np.array([cv2.imread(img_path) for img_path in X_test_paths_m])\n",
    "\n",
    "# Extract labels\n",
    "y_test_m = test_data_messidor[\"Retinopathy_Grade\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 910ms/step - loss: 4.8851 - accuracy: 0.2500\n",
      "Messidor Test Accuracy BASE images (without dropout): 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_base_m, test_accuracy_base_m = model_base.evaluate(X_test_m, y_test_m)\n",
    "print(\"Messidor Test Accuracy BASE images (without dropout):\", test_accuracy_base_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspected experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 132\u001b[0m\n\u001b[0;32m    127\u001b[0m     normalized_image \u001b[38;5;241m=\u001b[39m normalize_images(image)\n\u001b[0;32m    128\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(\n\u001b[0;32m    129\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(normalized_output_dir_train, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(image_path)),\n\u001b[0;32m    130\u001b[0m         normalized_image,\n\u001b[0;32m    131\u001b[0m     )\n\u001b[1;32m--> 132\u001b[0m     \u001b[43mX_train_paths_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_output_dir_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Normalize and save testing images\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_test_paths):\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons as tfa\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"train_split_key_transformation.csv\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "test_data = data[data[\"split\"] == \"test\"]\n",
    "\n",
    "# Define output directories for preprocessed and normalized images\n",
    "preprocessed_output_dir_train = \"preprocessed_images_train/\"\n",
    "preprocessed_output_dir_test = \"preprocessed_images_test/\"\n",
    "normalized_output_dir_train = \"normalized_images_train/\"\n",
    "normalized_output_dir_test = \"normalized_images_test/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(preprocessed_output_dir_train, exist_ok=True)\n",
    "os.makedirs(preprocessed_output_dir_test, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_train, exist_ok=True)\n",
    "os.makedirs(normalized_output_dir_test, exist_ok=True)\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, output_dir):\n",
    "    # Load and resize the image to 512x512\n",
    "    resized_image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(resized_image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    l_eq = clahe.apply(l)\n",
    "\n",
    "    # Merge back LAB channels\n",
    "    lab_eq = cv2.merge((l_eq, a, b))\n",
    "    enhanced_image = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Image noise removal using Gaussian filter\n",
    "    filtered_image = gaussian_filter(enhanced_image, sigma=1)\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    image_name = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, image_name)\n",
    "    cv2.imwrite(output_path, filtered_image)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# Define augmentation function\n",
    "def augment_image(image):\n",
    "    # Apply augmentation operations\n",
    "    image = tfa.image.rotate(image, tf.random.uniform([], -35, 35, dtype=tf.float32))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tfa.image.shear_x(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.shear_y(\n",
    "        image, tf.random.uniform([], -0.15, 0.15, dtype=tf.float32), 0\n",
    "    )\n",
    "    image = tfa.image.translate(\n",
    "        image, [tf.random.uniform([], -0.1, 0.1), tf.random.uniform([], -0.1, 0.1)]\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_images(images, factor=0.5):\n",
    "    normalized_images = []\n",
    "    for image in images:\n",
    "        # Ensure the image is in float32 format\n",
    "        image_float32 = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Compute mean intensity for all channels\n",
    "        mean_intensity = np.mean(image_float32)\n",
    "\n",
    "        # Compute scaling factor for intensity adjustment\n",
    "        scaling_factor = factor / mean_intensity\n",
    "\n",
    "        # Scale each channel independently\n",
    "        balanced_image = np.clip(image_float32 * scaling_factor, 0, 1)\n",
    "\n",
    "        # Convert image back to uint8 format\n",
    "        normalized_image = (balanced_image * 255).astype(np.uint8)\n",
    "\n",
    "        normalized_images.append(normalized_image)\n",
    "\n",
    "    return np.array(normalized_images)\n",
    "\n",
    "\n",
    "# Apply preprocessing to training images\n",
    "X_train_paths = []\n",
    "\n",
    "for img_name in train_data[\"Image name\"]:\n",
    "    image_path = \"train/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_train)\n",
    "    X_train_paths.append(output_path)\n",
    "\n",
    "# Apply preprocessing to testing images\n",
    "X_test_paths = []\n",
    "\n",
    "for img_name in test_data[\"Image name\"]:\n",
    "    image_path = \"test/\" + img_name + \".jpg\"\n",
    "    output_path = preprocess_image(image_path, preprocessed_output_dir_test)\n",
    "    X_test_paths.append(output_path)\n",
    "\n",
    "# Load preprocessed images\n",
    "X_train = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "X_train_paths_normalized = []\n",
    "X_test_paths_normalized = []\n",
    "\n",
    "# Normalize and save training images\n",
    "for i, image_path in enumerate(X_train_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_train, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "    X_train_paths_normalized.append(\n",
    "        os.path.join(normalized_output_dir_train, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "# Normalize and save testing images\n",
    "for i, image_path in enumerate(X_test_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    normalized_image = normalize_images(image)\n",
    "    cv2.imwrite(\n",
    "        os.path.join(normalized_output_dir_test, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "    X_test_paths_normalized.append(\n",
    "        os.path.join(normalized_output_dir_test, os.path.basename(image_path)),\n",
    "        normalized_image,\n",
    "    )\n",
    "\n",
    "X_train_new_norm = np.array([cv2.imread(img_path) for img_path in X_train_paths])\n",
    "X_test_new_norm = np.array([cv2.imread(img_path) for img_path in X_test_paths])\n",
    "\n",
    "# Apply augmentation to training images\n",
    "X_train_augmented = []\n",
    "\n",
    "for image in X_train_new_norm:\n",
    "    augmented_image = augment_image(image)\n",
    "    X_train_augmented.append(augmented_image)\n",
    "\n",
    "# Convert augmented images to numpy array\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_data[\"Retinopathy grade new\"].values\n",
    "y_test = test_data[\"Retinopathy grade new\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base learning rate for custom CNNs\n",
    "base_learning_rate = 1e-4\n",
    "# Maximum learning rate for custom CNNs\n",
    "max_learning_rate = 1e-1\n",
    "\n",
    "# Create an instance of SGD optimizer with initial learning rate\n",
    "optimizer = SGD(learning_rate=base_learning_rate, momentum=0.9)\n",
    "\n",
    "# Create class weights\n",
    "# Convert y_train to a hashable data type\n",
    "y_train_list = list(y_train)\n",
    "classes = np.unique(y_train_list)\n",
    "\n",
    "# Create class weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_list)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n",
    "# Define triangular schedule\n",
    "def triangular_schedule(epoch):\n",
    "    \"\"\"Triangular learning rate scheduler.\"\"\"\n",
    "    cycle_length = 10  # Define the length of a cycle\n",
    "    cycle = math.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = base_learning_rate + (max_learning_rate - base_learning_rate) * max(0, (1 - x))\n",
    "    return lr\n",
    "\n",
    "\n",
    "# When fitting the model, include the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(triangular_schedule)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train_augmented,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[lr_scheduler],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test_new_norm, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
